{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Andy (Tsz Kin) Chan <p> tkchanat |  @tkchanat |  Andy Chan |  Vancouver, BC</p> <p></p> <p>I am a rendering software engineer at Animal Logic Vancouver, developing in-house production renderer Glimpse. I love both offline and realtime graphics. My main interests are light transport and Monte Carlo path tracer, but any compute graphics related topics fascinate me too.</p> <p>This site is my dedicated graphics dump yard where you can (occasionally) find some useful knowledge. Happy diving!</p>"},{"location":"#experience","title":"Experience","text":"<p><sub>* Trademarks of the above are owned by their respective companies and publishers.</sub></p>"},{"location":"#publication","title":"Publication","text":"<p>Can You See the Heat? A Null-scattering Approach for Refractive Volume Rendering - SIGGRAPH 2023</p> <p>Basile Fraboni, Tsz Kin Chan, Thibault Vergne, Jakub Jeziorski</p> <p>[Project Page] | [Paper]</p>"},{"location":"#personal-projects","title":"Personal Projects","text":"<p>Migrating from old webpage </p>"},{"location":"#education","title":"Education","text":"<p>Hong Kong University of Science and Technology, HK (2016 - 2020)</p> <p>Bachelor of Engineering, Computer Science</p> <ul> <li>Advanced Computer Graphics</li> <li>Applied Statistics and Linear Algebra</li> <li>Computer Organization and Operating System</li> <li>Data Structure and Algorithms</li> <li>Software Engineering</li> </ul> <p>KTH Royal Institue of Technology, Sweden (2019 Jan - June)</p> <p>Exchange Study, Master's Program</p> <ul> <li>Computer Graphics and Interaction</li> <li>Music Communication and Music Technology</li> <li>Machine Learning</li> </ul>"},{"location":"#skills","title":"Skills","text":"Tools &amp; Application VSCode VisualStudio Git USD Unity UE4 Blender Programming Languages C++ Rust Python C# Javascript Graphics API Vulkan OpenGL DirectX 11 WebGL"},{"location":"notes/monte-carlo-integration/","title":"Monte Carlo Integration","text":""},{"location":"notes/monte-carlo-integration/#motivation","title":"Motivation","text":""},{"location":"notes/monte-carlo-integration/#quadrature-rules","title":"Quadrature rules","text":"<p>To integrate the function \\(I = \\int_{\\Omega}{f(x) dx}\\), where \\(\\Omega\\) is the domain of integration. If \\(f(x)\\) known to you and can be evaluated analytically, then you are good. But usually that's not the case, \\(f(x)\\) behaves like a blackbox (you give it an input and it spits out an output), it needs to be integrated numerically. </p> <p>Pretend you don't know the equation for the following function and you want to know its area bounded by it. In high school, we were taught that the integral can be approximated with infinitesimal rectangles, a.k.a. the rectangle rules. So let's put together a bunch of rectangles and crunch some numbers!</p> <p>As the number of rectangles grows, the closer it fills the target area. To which I can say with confidence the area is \\(\\pi/4\\) since it's obviously a quarter of a circle. </p> <p>There are more similar approaches to do integration and they are known as quatrature rules. The generalized form looks like this \\(\\hat{I} = \\sum_{i=1}^n w_i\\ f(x_i)\\). The weight \\(w_i\\) is defined differently in other rules, e.g. Midpoint rule, Trapezoid rule, Simpson's rule, etc. </p>"},{"location":"notes/monte-carlo-integration/#downside","title":"Downside","text":"<p>These kind of numerical approximations suffers from two major problems. </p>"},{"location":"notes/monte-carlo-integration/#high-frequency-signals","title":"High-frequency signals","text":"<p> Graphs like this can't be easily approximated with thick bars. To get closer to the real value, the bars must be really narrow which means more iterations to compute. </p>"},{"location":"notes/monte-carlo-integration/#high-dimensional-domains","title":"High-dimensional domains","text":"\\[ \\hat{I}=\\sum_{i_1=1}^n \\sum_{i_2=1}^n \\dots \\sum_{i_s=1}^n{w_{i_1} w_{i_2} \\dots w_{i_s} f(x_{i_1}, x_{i_2}, \\dots, x_{i_s})} \\] <p>where \\(s\\) is the dimension, \\(w_i\\) is the weights and \\(x_i\\) is the sample position in the domain. Which is always the case when solving light transport problems.</p>"},{"location":"notes/monte-carlo-integration/#definition","title":"Definition","text":"<p>As the name suggests, we can integrate functions with stochastic approaches through random sampling. With the Monte Carlo method, we can tap into the power of integrating arbitrary multi-dimensional functions!</p> \\[ F_N = \\frac{1}{N} \\sum_{i=1}^N{ \\frac{f(X_i)}{p(X_i)} } \\] <p>where \\(p(X_i)\\) is the probability density function (pdf). And we can verify that with sufficient samples, it will eventually converge to the expected value \\(I\\).</p> \\[ \\begin{align} E[F_N] &amp;= E[\\frac{1}{N}\\sum_{i=1}^N{\\frac{f(X_i)}{p(X_i)}}] \\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^N{\\int_{\\Omega}{\\frac{f(x)}{p(x)} p(x) d\\mu(x)}} \\\\ &amp;= \\int_{\\Omega}{f(x) d\\mu(x)} \\\\ &amp;= I \\end{align} \\] <p>It comes with the following benefits:</p> <ol> <li>It has a convergence rate of \\(O(\\frac{1}{\\sqrt{N}})\\) in any number of dimensions, which quadrature rule methods cannot achieve.</li> <li>Simplicity. All you need is two functions <code>sample()</code> and <code>eval()</code>, and occationally finding a suitable pdf.</li> </ol>"},{"location":"notes/monte-carlo-integration/#sampling-random-variables","title":"Sampling random variables","text":"<p>Generating uniform samples (e.g. \\(p(x)\\) is a constant) gurantees convergence but its rate is much slower. Ultimately, the samples should be drawn from a specific distribution such that most contribution to the integral is being extracted quickly from the domain. In other words, sampling carefully reduces the time to compute the ground-truth result.</p>"},{"location":"notes/monte-carlo-integration/#inverse-transform-sampling","title":"Inverse Transform Sampling","text":"<p>This is a method for generating random variables from a given probability distribution (pdf) by using its inverse cumulative distribution (cdf). Imagine the likelihood of picking a random variable \\(X\\) follows a normal distribution, and we want the samples to be drawn proportional to its likeliness. A cdf table is built by summing up the marginal distributions (It's totally fine that the pdf doesn't add up to 1, since the sample will be drawn from the range of cdf which can be normalized by its maximum value). Then uniform samples \\(U\\) are drawn in the inverse domain of cdf such that random variable \\(X\\) is picked with \\(X = cdf^{-1}(U)\\).</p> <p>Add 1 sample Add 10 samples Reset</p> <p>From the above example, we can see most samples are centered at the highest probability area while the tails on both sides will have lower sample count. Good thing with this method is that it can be easily extended to multi-dimensional cases, and stratifying samples in the domain helps improves exploring the entire domain. </p>"},{"location":"notes/monte-carlo-integration/#downside_1","title":"Downside","text":"<p>The pdf must be known, also building the cdf takes time and memory. And if the cdf cannot be inverted analytically, numerically computing the inverse mapping value (e.g. binary search) on every drawn samples is quite costly.</p> <p>Note: A more efficient sampling structure exists out there, and it's called the Alias Method, which samples can be drawn in constant \\(O(1)\\) time. // TODO: Make a page about this</p>"},{"location":"notes/monte-carlo-integration/#rejection-sampling","title":"Rejection Sampling","text":"<p>When the pdf is difficult to sample, we can instead sample from a simpler density \\(q\\) such that \\(p(x) \\le M q(x)\\), where \\(M\\) is a scaling constant. For instance, you want to integrate the area of an arbitrary shape but directly sampling the shape is hard. However, you know it's much easiler to draw samples from a box. So let's throw some dots onto our sample space!</p> <p>Start Reset</p> <p>The sampling space is defined as the tight bounding box of the shape, since drawing samples from a square is much simpler. We know the probability \\(p\\) of picking a sample point inside the shape is always less than the density \\(q\\) (it's completely enclosed within the bound), it's eligible to use this strategy to draw a sample. To draw one:</p> <ol> <li>Sample \\(X_i\\) according to \\(q\\) (draw a point inside the square)</li> <li>Sample \\(U_i\\) uniformly on \\([0, 1]\\)</li> <li>If \\(U_i \\le p(X_i) / (Mq(X_i))\\), return the sample \\(X_i\\)</li> <li>Else, repeat 1</li> </ol> <p>In the above case, \\(p(X_i)\\) is \\(\\frac{1}{area}\\) when the point is drawn inside the shape else zero, and \\(q(X_i)\\) is always \\(\\frac{1}{64}\\) because we are uniformly sampling a 8x8 square. Given that \\(U_i\\) has a trivial probability of being 0, we can safely assume that all valid samples \\(X_i\\) are located inside the shape. Thus we know they are good samples.</p>"},{"location":"notes/monte-carlo-integration/#importance-sampling","title":"Importance Sampling","text":"<p>\\(N=\\) \\(\\int{f(x)}dx =\\) Approx \\(=\\) Start Reset</p>"},{"location":"notes/monte-carlo-integration/#bsdf-sampling","title":"BSDF Sampling","text":"\\[ p(w_i') \\propto f_s(\\mathbf{x}', w_i' \\rightarrow w_o') \\] <p>Depending on the surface properties, there are certain directions (\\(w_i'\\)) the BSDF favors after an interaction. To get the more contribution from the function, samples have to be drawn proportional to the \\(f_s\\)'s shape. Because BSDF samples are drawn inside the solid angle domain, the probability \\(p(w_i')\\) is also measured in solid angle.</p>"},{"location":"notes/monte-carlo-integration/#light-sampling","title":"Light Sampling","text":"<p>In case when BSDF failed to find a significant contribution, other words the outgoing ray direction missed the light source, light sampling then comes into play to provide as a backup strategy.</p> \\[ L_o(\\mathbf{x}'\\rightarrow\\mathbf{x}'') = \\int_{\\mathcal{M}}{f_s(\\mathbf{x}\\rightarrow\\mathbf{x}'\\rightarrow\\mathbf{x}'')L_e(\\mathbf{x}\\rightarrow\\mathbf{x}')G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')dA(\\mathbf{x})} \\] <p>\\(G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\) often refers as the geometric term, which was introduced because of change of variables. When changing from projected solid angle \\(d\\sigma^\\bot(w_i')\\) to area measure \\(dA(\\mathbf{x})\\), it is required to have this term to normalize the integral:</p> \\[ G(\\mathbf{x}\\leftrightarrow\\mathbf{x}') = V(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\frac{cos(\\theta_o)cos(\\theta_i')}{||\\mathbf{x}-\\mathbf{x}'||^2} \\] <p>To importance sample the light instead of solid angle, the density \\(p(\\mathbf{x})\\) is predetermined since it is just the probability of picking a point on the manifold surface, i.e. \\(p(\\mathbf{x})=\\frac{1}{Area}\\). </p>"},{"location":"notes/monte-carlo-integration/#transformation-of-space-conversion-of-domain","title":"Transformation of Space / Conversion of Domain","text":"\\[ p(\\mathbf{x})=p(w_i')\\frac{d\\sigma^\\bot(w_i')}{dA(\\mathbf{x})}=p(w_i')\\frac{|cos(\\theta_o)cos(\\theta_i')|}{||\\mathbf{x}-\\mathbf{x}'||^2} \\]"},{"location":"notes/monte-carlo-integration/#multiple-importance-sampling","title":"Multiple Importance Sampling","text":"<p>Next event estimation can be seen as a multiple importance sampling<sup>1</sup> approach of integrating the radiance since it combines two sampling strategies, BSDF sampling and light sampling, often called as direct and indirect lighting. </p> <ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/sampling-techniques/","title":"Sampling Techniques","text":"<p>Puseudo-random noise usually don't produce good uniform samples. It often produces clusters and void within the sample space. That means the sample space is not well explored, either wasting samples on similar areas or even complete ignoring some subregions. And this scales to any dimensions.</p> <p></p>"},{"location":"notes/sampling-techniques/#stratified-sampling","title":"Stratified Sampling","text":"<p>Instead of recklessly scatter points around, how about subdiving the domain \\(\\Omega\\) into non-overlapping regions \\(\\Omega_1, \\dots, \\Omega_n\\). Noted that the union of them must cover the whole domain.</p> \\[ \\bigcup_{i=1}^n{\\ \\Omega_i} = \\Omega \\] <p></p> <p>This works pretty well in low-dimensional integration problems. However unfortunately, this suffers the same problem as the quadrature rules where it won't perform well in high frequency signals.</p>"},{"location":"notes/sampling-techniques/#low-discrepancy-sequences","title":"Low-Discrepancy Sequences","text":"<p>If randomness are difficult to control, how about herding the points to position in a deterministic pattern such that they are almost equal distance to each other? In other words, how do we get an equidistribution of samples?</p>"},{"location":"notes/sampling-techniques/#defining-discrepancy","title":"Defining Discrepancy","text":"<p>Imagine a \\(s\\)-dimentional unit cube \\(\\mathbb{I}^s = [0, 1)^s\\), with a point set \\(P = {x_1, x_2, \\dots, x_N} \\in \\mathbb{I}^s\\). We define the point set discrepancy<sup>1</sup> \\(\\mathcal{D}(J, P)\\) as follows:</p> \\[ \\mathcal{D}(J, P) = \\left|\\frac{A(J)}{N} - V(J)\\right| \\] <p>You can think of \\(\\mathcal{D}(J, P)\\) as the proportion of points inside a sub-interval \\(J\\), where \\(A(J)\\) is the number of points \\(x_i \\in J\\) and \\(V(J)\\) is the volume of \\(J\\). </p> <p>The worst-case discrepancy is called the star-discrepancy and is defined as:</p> \\[ \\mathcal{D^*}(N)=\\sup_{J\\in\\mathbb{I^s}}{|\\mathcal{D}(J;P)|} \\] <p>A good sequence should minimize such star-discrepancy \\(\\mathcal{D^*}(N)\\) to be qualified as low-discrepancy. Perhaps it's easier to visualize the terms in a diagram.</p> <p>\\(N=\\) \\(\\mathcal{D}(A, P) =|\\)\\(\\frac{A(A)}{N}-V(A)\\)\\(|=\\) \\(\\mathcal{D}(B, P) =|\\)\\(\\frac{A(B)}{N}-V(B)\\)\\(|=\\) \\(\\mathcal{D}(C, P) =|\\)\\(\\frac{A(C)}{N}-V(C)\\)\\(|=\\) \\(\\mathcal{D^*}(P) =\\)</p> <p>Generate Point Set Reset</p>"},{"location":"notes/sampling-techniques/#halton-sequence","title":"Halton Sequence","text":"<p>One of the well-known low discrepancy sequences is generated using the radical inverse of numbers. They are called radical inverse sequence, and is defined as:</p> \\[ \\phi_b(i) = \\sum_{k\\ge 0}{d_{i,k}\\ b^{-1-k}} \\] <p>where \\(b\\) is the base and \\(d_k\\) is the \\(k\\)-th digit in the \\(b\\)-ary expansion of \\(n\\). To generate a sequence for \\(b=2\\), first represent the natural numbers in binary, then revert the digits and take its inverse. i.e.</p> \\[ \\begin{align} &amp;\\quad \\phi_2(1),\\phi_2(2),\\phi_2(3),\\phi_2(4),\\phi_2(5),\\phi_2(6),\\dots \\\\ &amp;=0.1_2, 0.01_2, 0.11_2, 0.001_2, 0.101_2, 0.011_2,\\dots \\\\ &amp;=\\frac12,\\frac14,\\frac34,\\frac18,\\frac58,\\frac38,\\dots \\\\ \\end{align} \\] <p>This is also known as the van der Corput sequence<sup>2</sup>, a specialized one-dimensional radical inverse sequence. The generalized form is called the Halton sequence, that is scalable to higher dimensions. To generate points in \\(N\\)-dimension, simply pick a different base from the prim number series \\({2, 3, 5, 7, 11, \\dots}\\). Scratchapixel has a more in-depth explanation, feel free to give it a read! </p> <p>Start Reset</p>"},{"location":"notes/sampling-techniques/#sobol-sequence","title":"Sobol Sequence","text":""},{"location":"notes/sampling-techniques/#progressive-multi-jittered-sample-sequence","title":"Progressive Multi-Jittered Sample Sequence<sup>3</sup>","text":"<ol> <li> <p>Dalal, I., Stefan, D., &amp; Harwayne-Gidansky, J. (2008). Low discrepancy sequences for Monte Carlo simulations on reconfigurable platforms. In 2008 International Conference on Application-Specific Systems, Architectures and Processors (pp. 108\u2013113).\u00a0\u21a9</p> </li> <li> <p>van der Corput, J.G. (1935), \"Verteilungsfunktionen (Erste Mitteilung)\" (PDF), Proceedings of the Koninklijke Akademie van Wetenschappen te Amsterdam (in German), 38: 813\u2013821, Zbl 0012.34705\u00a0\u21a9</p> </li> <li> <p>Christensen, P., Kensler, A., &amp; Kilpatrick, C. (2018). Progressive Multi-Jittered Sample Sequences. Computer Graphics Forum.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/BSDF/microfacet/","title":"Microfacet Theory","text":"<p>There is a well-written paper by Eric Heitz<sup>1</sup> explaining every details about the microfacet theory. I highly recommend everyone to read it to get a thorough understanding from beginning to end. The stuff I write is just an abstract plus my own understanding (prone to misinformation). I am happy to correct this page anytime. </p>"},{"location":"notes/BSDF/microfacet/#radiance-on-surface","title":"Radiance on Surface","text":"<p>Here is the definition of radiance from wikipedia.</p> <p>In radiometry, radiance is the radiant flux emitted, reflected, transmitted or received by a given surface, per unit solid angle per unit projected area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>It matches the one defined in Veach's thesis, which is written as:</p> \\[ \\begin{align*} L(\\omega, \\mathbf{x}) &amp;= \\frac{d^2\\Phi(\\omega)}{dA^\\bot_{\\omega}(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{dA(\\mathbf{x})\\ d\\sigma^\\bot_{\\omega}(\\omega)} \\end{align*} \\] <p>The \\(A^\\bot_{\\omega}\\) here means the area projected from the hypothetical surface onto the observed direction \\(\\omega\\). For a real surface, the projected area measure \\(dA^\\bot_{\\omega}\\) in a real surface can be written as \\(\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\). Even better, the dot product \\(\\left|\\omega\\cdot \\mathbf{n}\\right|\\) can be transfered into the solid angle measure \\((\\sigma_\\omega \\rightarrow \\sigma^\\bot_{\\omega})\\), which can be beneficial sometimes if we want to leverage that property during integration, e.g. Stratified sampling of projected spherical caps, EGSR 2018.</p> <p>Extending this to a microfacet model, where every micronormals \\(\\omega_m\\) has its own direction, we need to integrate that over a small patch of surface \\(\\mathcal{M}\\). </p> \\[ L(\\omega_o, \\mathcal{M}) = \\frac{\\int_\\mathcal{M}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{M})} \\] <p>For every infinitesimal points \\(\\mathbf{x}\\in\\mathcal{M}\\), its local radiance towards the observing direction is \\(A^\\bot_{\\omega_o}L(\\omega_o,\\mathbf{x})\\). So the integrated result will be the total amount of watts per steradian \\([W\\cdot sr^{-1}]\\) that is radiating off to the observing direction \\(\\omega_o\\). The purpose of the denominator \\(A^\\bot_{\\omega_o}(\\mathcal{M})\\) which represents the total projected area of \\(\\mathcal{M},\\) is here merely to normalize the entire thing back to per unit area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>Fun little fact, for marcosurface \\(\\mathcal{G}\\) which its normal doesn't vary within the domain, it's radiance is no difference than the original definition of radiance.</p> \\[ \\begin{align*} L(\\omega_o, \\mathcal{G}) &amp;= \\frac{\\int_\\mathcal{G}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= \\frac{A^\\bot_{\\omega_o}(\\mathcal{G})\\ L(\\omega_o, \\mathbf{x})}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= L(\\omega_o, \\mathbf{x}) \\end{align*} \\]"},{"location":"notes/BSDF/microfacet/#normal-distribution","title":"Normal Distribution","text":"<p>Spatial integral isn't ideal to work with because the whole microfacet theory is based on a statistical model, it's not necessarily tied to an actual spatial representation. That being said, we need a way to convert that integral into the solid angle domain. Luckily, the distribution of normals is able to provide just that.</p> \\[ D(\\omega) = \\int_\\mathcal{M}{\\delta_\\omega(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x}} \\] <p>It is expressed in square meters per steradian \\([m^2\\cdot sr^{-1}]\\), representing the density of normals that aligns a given direction \\(\\omega\\) over the surface \\(\\mathcal{M}\\), hence the dirac delta function \\(\\delta_\\omega\\). To better understand this magical function, imagine counting the number of micronormals in the patch that are pointing to the direction \\(\\omega\\) within the domain space \\(\\mathcal{M}\\). Its sum will be the area covering the selected population of normals, and this is where the area term \\(m^2\\) comes from. This adds on top of the fact that dirac delta always has the inverse dimension of its argument, which is in \\(sr\\) here, so its unit is per steradian \\(sr^{-1}\\). </p> <p>Let's talk about an interesting property of the integral of normal distribution. Over any arbitrary solid angle region \\(\\Omega'\\subset\\Omega\\) on the unit sphere, it always gives the total covered area whose normals lie in \\(\\Omega'\\).</p> \\[ \\int_\\mathcal{M'}1\\ d\\mathbf{x} = \\int_{\\Omega'} D(\\omega_m)\\ d\\omega_m \\] <p>This makes \\(\\int_\\Omega D(\\omega_m)\\ d\\omega_m\\) the total area of the microsurface \\(\\mathcal{M}\\). Magical, right? Now any attempts of integrating a function of the microsurface normal \\(\\omega_m\\) spatially over \\(\\mathcal{M}\\) can be converted into a statistial integral:</p> \\[ \\int_\\mathcal{M}f(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x} = \\int_\\Omega f(\\omega_m)\\ D(\\omega_m)\\ d\\omega_m \\]"},{"location":"notes/BSDF/microfacet/#geometric-attenuation","title":"Geometric Attenuation","text":"<p>\\(G_1(\\omega_o, \\omega_m)\\) is the statistical masking function of micronormal \\(\\omega_m\\). </p> <p>\\(G_1(\\omega_i, \\omega_m)\\) is the statistical shadowing function of micronormal \\(\\omega_m\\)</p> <p>\\(G(\\omega_i, \\omega_o)\\) is the shadowing-masking term</p>"},{"location":"notes/BSDF/microfacet/#various-models","title":"Various Models","text":""},{"location":"notes/BSDF/microfacet/#ggx","title":"GGX","text":"<p>GGX<sup>2</sup> is a well-known geometry-based microfacet BSDF model for rough surfaces. </p> \\[D(\\mathbf{h})=\\frac{\\alpha^2\\chi^+(\\left&lt;\\mathbf{h}\\cdot\\mathbf{n}\\right&gt;)}{\\pi cos^4\\theta_\\mathbf{h}(\\alpha^2+tan^2\\theta_\\mathbf{h})^2}\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=\\chi^+\\left(\\frac{\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{v}\\cdot\\mathbf{n}\\right&gt;}\\right)\\quad\\frac{2}{1+\\sqrt{1+\\alpha^2tan^2\\theta_\\mathbf{v}}}\\]"},{"location":"notes/BSDF/microfacet/#schlicks-approximation","title":"Schlick's Approximation","text":"<p>Christophe Schlick proposed an inexpensive and efficient model<sup>3</sup> in attempt to capture the microfacet properties. This is probably the most popular go-to model aside from the Cook-Torrance model, usually used in real-time graphics since it has less mathematical operations. Both \\(F\\) and \\(G\\) terms are inspired from the Cook-Torrance model, while the \\(D\\) term is derived from the Beckmann distribution.</p> \\[F(\\mathbf{v},\\mathbf{h})=f_0+(1-f_0)(1-\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;)^5\\] \\[D(\\mathbf{h})=\\frac{\\alpha^3x}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(\\alpha x^2-x^2+\\alpha^2)^2},\\quad x=\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;+\\alpha-1\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=G_1(\\mathbf{l})\\ G_1(\\mathbf{v}),\\quad G_1(\\mathbf{v})=\\frac{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(1-k)+k},\\quad k=\\sqrt{\\frac{\\pi}{2\\alpha^2}}\\]"},{"location":"notes/BSDF/microfacet/#unreal-4-approximation","title":"Unreal 4 Approximation","text":"<p>Epic Games<sup>4</sup> mostly adopts the formulations of \\(F\\) and \\(G\\) from the Schlick's model with slight modifications, but picked the \\(D\\) in the Disney's GGX model. These are obviously an approximation on top of an approximation. Every decisions made here are sacrificing minor visual error for faster computations.</p> \\[F(\\mathbf{v},\\mathbf{h})=f_0+(1-f_0)\\cdot2^{(-5.55473\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;-6.98316)\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;}\\] \\[D(\\mathbf{h})=\\frac{\\alpha^2}{\\pi(\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;^2(\\alpha^2-1)+1)^2}\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=G_1(\\mathbf{l})\\ G_1(\\mathbf{v}),\\quad G_1(\\mathbf{v})=\\frac{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(1-k)+k},\\quad k=\\frac{(\\alpha+1)^2}{8}\\] <ol> <li> <p>Heitz E. (2014). Understanding the masking-shadowing function in microfacet-based BRDFs. Journal of Computer Graphics Techniques, 3(2), 32-91. \u21a9</p> </li> <li> <p>Walter B., Marschner S. R., Li H., &amp; Torrance K. E. (2007, June). Microfacet models for refraction through rough surfaces. In Proceedings of the 18th Eurographics conference on Rendering Techniques (pp. 195-206). \u21a9</p> </li> <li> <p>Schlick C. (1994, August). An inexpensive BRDF model for physically\u2010based rendering. In Computer graphics forum (Vol. 13, No. 3, pp. 233-246). Edinburgh, UK: Blackwell Science Ltd. \u21a9</p> </li> <li> <p>Karis B, Epic Games. (2013). Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 4(3), 1. \u21a9</p> </li> </ol>"},{"location":"notes/mathematics/running-statistics/","title":"Running Statistics","text":"<p>People often use running statistics in finance to model a never-ending sequence of data. I use it mostly for stochastic Monte Carlo simulation. </p>"},{"location":"notes/mathematics/running-statistics/#welfords-algorithm","title":"Welford's Algorithm","text":"\\[ \\begin{align*} \\bar{x}_n &amp;= \\frac{1}{n}\\sum_{i=1}^n{x_i} &amp; &amp;= \\frac{(n-1)\\bar{x}_{n-1}+x_n}{n}\\\\ \\sigma_n^2 &amp;= \\frac{1}{n}\\sum_{i=1}^n{(x_i-\\bar{x}_n)^2} &amp; &amp;= \\frac{(n-1)\\sigma_{n-1}^2+(x_n-\\bar{x}_{n-1})(x_n-\\bar{x}_n)}{n}\\\\ \\end{align*} \\] <p>Precision proof version:</p> \\[ \\begin{align*} \\bar{x}_n &amp;= \\bar{x}_{n-1} + \\frac{x_n-\\bar{x}_{n-1}}{n}\\\\ \\sigma_n^2 &amp;= \\sigma_{n-1}^2 + \\frac{(x_n-\\bar{x}_{n-1})(x_n-\\bar{x}_n) - \\sigma_{n-1}^2}{n}\\\\ \\end{align*} \\]"},{"location":"notes/mathematics/running-statistics/#rust-example","title":"Rust Example","text":"<pre><code>use std::ops::{Add, Div, Mul, Sub};\n\n#[derive(Default)]\nstruct WelfordOnlineStats&lt;T&gt; {\n    pub mean: T,\n    pub variance: T,\n    pub count: usize,\n}\nimpl&lt;T&gt; WelfordOnlineStats&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt;\n        + Sub&lt;Output = T&gt;\n        + Mul&lt;Output = T&gt;\n        + Div&lt;f32, Output = T&gt;\n        + Copy\n        + Clone,\n{\n    fn add(&amp;mut self, value: T) {\n        if self.count == 0 {\n            self.mean = value;\n            self.count += 1;\n        } else {\n            self.count += 1;\n            let diff: T = value - self.mean;\n            self.mean = self.mean + diff / self.count as f32;\n            self.variance = self.variance\n                + (diff * (value - self.mean) - self.variance) / (self.count - 1) as f32;\n        }\n    }\n}\n</code></pre>"},{"location":"notes/mathematics/spherical-harmonics/","title":"Spherical Harmonics","text":"<p>If you know what Fourier series are, spherical harmonics are very similar periodic basis functions. But rather on the time domain, it's defined on the surface of a sphere \\(S^2\\). </p>"},{"location":"notes/mathematics/spherical-harmonics/#analogy-to-fourier-series","title":"Analogy to Fourier series","text":"<p>Let's start small. Fourier series is defined as the expansion of a periodic function into a sum of trigonometric functions<sup>1</sup>. So it's just a sum of basis wave functions (i.e. \\(\\sin\\) and \\(\\cos\\)). It's often expressed in the time domain, which is a single variable only. </p> <p>Here is the square wave fourier series expansion up to \\(n\\) terms. It's defined as:</p> \\[ y=\\frac{4}{\\pi} \\sum_{k=1}^n {\\frac{\\sin((2k-1)x)}{2k-1}} \\] <p>Now imagine transferring the square waves onto a unit circle \\(S^1\\). I guess you could call it \"Circular Harmonics\"<sup>2</sup>, if you really want to give it a name. </p> <p>But at the end, this is still a Fourier series, just with a different parameterization. Noted that \\(x\\) is now mapped to the angular part \\(\\theta \\in [0, 2\\pi)\\), and \\(y\\) is mapped to the radial part \\(r\\) in a polar coordinate system. There is no significant difference from our first equation.</p> \\[ r = \\frac{4}{\\pi} \\sum_{k=1}^n {\\frac{\\sin((2k-1)\\theta)}{2k-1}} \\] <p>I purposely color the positive and negative lobes in two different colors, because in one sine function cycle, \\([0, \\pi]\\) is positive and \\([\\pi, 2\\pi]\\) is negative. When it's wrapped around a circle, they become distinct lobes carrying opposite sign.</p> <p>Let's break the series back to its basis functions. That is, without the scaling, we now truly obtained the \"Circular Harmonics\" bases. And here is how they look.</p> \\[ r=\\sin(k\\theta) \\] <p>Looks a lot like spherical harmonics, right? With these basis functions, they can theoretically form any functions if it's an infinite series expansion. But realistically, in most applications, only a few of them are needed to approximate/fit a smooth circular function. </p> <p>Circular Harmonics Rotation</p> <p>One interesting observation about rotating these basis functions, is that the choice of the oscillating/trigonometric function. Recall that \\(\\sin(\\theta)=\\cos(\\frac{\\pi}{2}-\\theta)\\), meaning cosine is just an off-phase sine. Therefore, substituding the sine to cosine is all we need to rotate it by \\(90^{\\circ}\\).</p>"},{"location":"notes/mathematics/spherical-harmonics/#derivation-of-spherical-harmonics","title":"Derivation of Spherical Harmonics","text":"<p>Spherical harmonics originate from solving Laplace's equation in the spherical domains. Functions that are solutions to Laplace's equation are called harmonics<sup>3</sup>. </p> <p>To recap Laplace's equation is to solve for a function \\(f\\) where its divergence of gradient is 0, e.g. \\(\\nabla^2f=0\\). In Cartesian coordinates, the three-dimensional Laplacian is defined as:</p> \\[ \\nabla^2 f = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2} + \\frac{\\partial^2f}{\\partial z^2} \\] <p>In spherical coordinates \\(x=r\\sin\\theta\\cos\\phi,\\ y=r\\sin\\theta\\sin\\phi,\\ z=r\\cos\\theta\\), it became: (full derivation)</p> \\[ \\begin{align*} \\nabla^2f &amp;= \\frac{1}{r^2\\sin\\theta}   \\left(     \\frac{\\partial}{\\partial r} r^2 \\sin\\theta \\frac{\\partial}{\\partial r} + \\frac{\\partial}{\\partial \\theta} \\sin\\theta \\frac{\\partial}{\\partial\\theta} + \\frac{\\partial}{\\partial\\phi} \\csc\\theta \\frac{\\partial}{\\partial\\phi}   \\right) \\\\   &amp;= \\frac{1}{r^2}   \\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial f}{\\partial r}\\right) +    \\frac{1}{r^2\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial f}{\\partial\\theta}\\right) +    \\frac{1}{r^2\\sin^2\\theta} \\frac{\\partial^2 f}{\\partial\\phi^2} \\end{align*} \\] <p>To separate the radial part (\\(r\\)) from the angular part (\\(\\theta, \\phi\\)), we need to let \\(f(r, \\theta, \\phi)=R(r)Y(\\theta, \\phi)\\) and perform what's called a separation of variables.</p> \\[ \\begin{align*}     &amp;\\ \\nabla^2 (R(r)Y(\\theta, \\phi))\\\\    = &amp;\\ \\frac{1}{r^2} \\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)Y(\\theta, \\phi)}{\\partial r}\\right) +    \\frac{1}{r^2\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial R(r)Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{r^2\\sin^2\\theta} \\frac{\\partial^2 R(r)Y(\\theta, \\phi)}{\\partial\\phi^2}\\\\   = &amp;\\cancel{\\frac{1}{r^2}}\\ Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) +    \\cancel{\\frac{1}{r^2}}R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)\\\\   = &amp;\\ Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) + R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) \\end{align*} \\] <p>Multiply both side by \\(\\frac{1}{R(r)Y(\\theta, \\phi)}\\):</p> \\[ \\begin{align*}   Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) + R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) &amp;= 0   \\\\   \\frac{\\cancel{Y(\\theta, \\phi)}}{R(r)\\cancel{Y(\\theta, \\phi)}}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right) + \\frac{\\cancel{R(r)}}{\\cancel{R(r)}Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) &amp;= \\frac{1}{R(r)Y(\\theta, \\phi)} \\cdot 0   \\\\   \\underbrace{\\frac{1}{R(r)}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)}_{r\\text{-dependent}} + \\underbrace{\\frac{1}{Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)}_{\\theta\\phi\\text{-dependent}} &amp;= 0 \\end{align*} \\] <p>Now we can introduce a separation constant \\(\\ell(\\ell+1)\\) such that both \\(r\\text{-dependent}\\) and \\(\\theta\\phi\\text{-dependent}\\) parts still adds up to 0, satisfying the original Laplace's equation.</p> \\[ \\begin{cases}   \\frac{1}{R(r)}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)   = \\ell(\\ell+1)\\\\   \\frac{1}{Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)   =   -\\ell(\\ell+1) \\end{cases} \\] <p>Then we have successfully obtained two separated ordinary differential equations, the radial part \\(\\eqref{1}\\) and the angular part \\(\\eqref{2}\\).</p> \\[ \\begin{align*}   &amp;\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right) = \\ell(\\ell+1)R(r) &amp;\\tag{1}\\label{1}\\\\   &amp;\\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2} = -\\ell(\\ell+1)Y(\\theta, \\phi) &amp;\\tag{2}\\label{2} \\end{align*} \\] <p>Spherical harmonics arises from the angular part \\(Y(\\theta, \\phi)\\) of a spherical Laplace's equation. Those \"harmonics\" are forms an infinite set of solutions that satisfy the angular part of the Laplace's equation. Therefore, the radial part \\(R(r)\\) can be omitted for now.</p> Linearity of Laplace's equation and relation to SH <p>The Laplace operator \\(\\nabla^2\\) is a linear operator since it satisfies the superposition principle, where the sum of any two solutions is also a solution. That is, \\(\\nabla^2(a\\theta + b\\phi) = a\\nabla^2(\\theta) + b\\nabla^2(\\phi)\\). The only prerequisite is that \\(\\nabla^2(\\theta)=0\\) and \\(\\nabla^2(\\phi)=0\\), and thus proofing its linearity (i.e. \\(a\\cdot0+b\\cdot0=0\\)).</p> \\[ \\underbrace{\\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}}_{\\nabla^2 f} = \\underbrace{-\\ell(\\ell+1)}_{\\text{eigenvalue}(\\lambda)}\\underbrace{Y(\\theta, \\phi)}_{\\text{eigenfunction}(f)} \\] <p>Taking equation \\((2)\\) and simplify it to \\(\\nabla^2(Y(\\theta, \\phi)) = -\\ell(\\ell+1)Y(\\theta, \\phi)\\). It's not hard to see that this takes the form of \\(Df=\\lambda f\\), where \\(D\\) is a linear operator<sup>4</sup><sup>5</sup>. And just so happened the Laplace operator \\(\\nabla^2\\) is also a linear operator! So, our separation constant \\(-\\ell(\\ell+1)\\) are actually eigenvalues and the spherical harmonics \\(Y(\\theta, \\phi)\\) are indeed eigenfunctions.</p> <p>By writing \\(Y(\\theta, \\phi)=\\Theta(\\theta)\\Phi(\\phi)\\), we are going to perform the separation of variable again. That gives us:</p> \\[ \\begin{align*} \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial (\\Theta(\\theta)\\Phi(\\phi))}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 (\\Theta(\\theta)\\Phi(\\phi))}{\\partial\\phi^2} &amp;= -\\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi))\\\\ \\frac{\\Phi(\\phi)}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\frac{\\Theta(\\theta)}{\\sin^2\\theta} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2} + \\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi)) &amp;= 0\\\\ \\end{align*} \\] <p>Multiplying both sides by \\(\\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\) <sup>6</sup>:</p> \\[ \\begin{align*} \\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\left(\\frac{\\Phi(\\phi)}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\frac{\\Theta(\\theta)}{\\sin^2\\theta} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2} + \\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi))\\right) &amp;= \\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\cdot0\\\\ \\underbrace{\\frac{\\sin\\theta}{\\Theta(\\theta)} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\ell(\\ell+1)\\sin^2\\theta}_{\\theta\\text{-dependent}} + \\underbrace{\\frac{1}{\\Phi(\\phi)} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2}}_{\\phi\\text{-dependent}} &amp;= 0 \\tag{3}\\label{3}\\\\ \\end{align*} \\] <p>This time let \\(m^2\\) be the separation constant:</p> \\[ \\begin{align*}   &amp;\\frac{\\sin\\theta}{\\Theta(\\theta)} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\ell(\\ell+1)\\sin^2\\theta = m^2 &amp;\\tag{4}\\label{4} \\\\   &amp;\\frac{1}{\\Phi(\\phi)}{\\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2}} = -m^2 &amp;\\tag{5}\\label{5} \\end{align*} \\] <p>Fortunately, \\(\\eqref{5}\\) already has solutions of \\(\\Phi(\\phi)=e^{\\pm im\\phi}\\), which is the two-dimensional angular Laplacian. Substituding it back into \\(\\eqref{3}\\) and solve for the \\(\\theta\\text{-dependent}\\) portion, we get the solutions of \\(\\Theta(\\theta)=P_\\ell^m(\\cos\\theta)\\) in the form of a Legendre polynomial<sup>6</sup><sup>7</sup> \\(P_\\ell^m(x)\\). (1)</p> <ol> <li>I have zero clue on this part about Legendre polynomials. Come back here when I am prepared.</li> </ol> \\[ P_\\ell^m(x) = \\frac{(-1)^m}{2^\\ell \\ell!}(1-x^2)^\\frac{m}{2} \\frac{\\partial^{\\ell+m}}{\\partial x^{\\ell+m}} (x^2-1)^\\ell \\] <p>Noted that this formula must have \\(\\ell \\geq 0\\) and \\(m\\) being integers such that \\(|m| \\leq \\ell\\). </p> <p>With that, we get the solution to \\(\\eqref{2}\\), which is the spherical harmonics \\(Y(\\theta, \\phi)=P_\\ell^m(\\cos\\theta)e^{\\pm im\\phi}\\). And just like vectors, to enforce orthonormality, a normalization factor is required to make independent spherical harmonics orthonormal.</p> \\[ Y_\\ell^m(\\theta, \\phi) = \\underbrace{\\sqrt{\\frac{2\\ell+1}{4\\pi}\\frac{(\\ell-m)!}{(\\ell+m)!}}}_{\\text{normalization factor}} P_\\ell^m(\\cos\\theta)e^{\\pm im\\phi} \\] <p>\\(Y(\\theta, \\phi)\\) denotes the whole sets of infinitely many solutions to the angular Laplacian. With this notation \\(Y_\\ell^m(\\theta, \\phi)\\) means it's a specific spherical harmonic with order \\(\\ell\\) and degree \\(m\\). </p> <ol> <li> <p>Fourier series (2024). In Wikipedia. https://en.wikipedia.org/wiki/Fourier_series \u21a9</p> </li> <li> <p>Circular Harmonics: Digging in circles (2021). Jon Vald\u00e9s. https://valdes.cc/articles/ch.html \u21a9</p> </li> <li> <p>Spherical harmonics (2024). In Wikipedia. https://en.wikipedia.org/wiki/Spherical_harmonics \u21a9</p> </li> <li> <p>Eigenfunction (2024). In Wikipedia. https://en.wikipedia.org/wiki/Eigenfunction \u21a9</p> </li> <li> <p>All You Need to Know about Spherical Harmonics. Mathcube. https://www.cantorsparadise.com/all-you-need-to-know-about-spherical-harmonics-29ff76e74ad5 \u21a9</p> </li> <li> <p>Spherical Harmonic. Weisstein, Eric W. From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/SphericalHarmonic.html \u21a9\u21a9</p> </li> <li> <p>Associated Legendre Polynormial. Weisstein, Eric W. From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/AssociatedLegendrePolynomial.html \u21a9</p> </li> </ol>"},{"location":"notes/simulation/force-based/","title":"Force Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/impulse-based/","title":"Impulse Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/numerical-methods/","title":"Numerical Methods","text":"<p>In this chapter, I will cover different methods to integrate differential equations (ODE) numerically. Since the integrand is evaluated numerically, I will replace every \\(=\\) to be \\(\\approx\\) to make it clear it's just an estimation only.</p> <p>// TODO: investigate what is consistency and order in numerical methods</p>"},{"location":"notes/simulation/numerical-methods/#euler-method","title":"Euler Method","text":"<p>This is a first-order numerical method to solve ordinary differential equations (ODEs) with a given initial value. It allows us to approximate a nearby point on the curve by moving a short distance \\(\\Delta t\\) along a line tangent, that is the first-order derivative \\(f'(t)\\), to the curve \\(f(t)\\). To write that down:</p> \\[ \\begin{cases} f'(t) &amp;\\approx \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t) \\Delta t \\end{cases} \\] <p>Rewriting it in an iterative way, we can find the next position a particle \\(\\mathbf{x}_{n+1}\\) with its current position \\(\\mathbf{x}_n\\):</p> \\[ \\mathbf{x}_{n+1} \\approx \\mathbf{x}_n + \\dot{\\mathbf{x}}(t) \\Delta t \\]"},{"location":"notes/simulation/numerical-methods/#implicit-euler-method","title":"Implicit Euler Method","text":"<p>The implicit here means instead of computing directly the approximation \\(\\mathbf{x}_{n+1}\\) from \\(\\mathbf{x}_n\\), we need to solve an implicit equation:</p> \\[ \\begin{cases} f'(t) &amp;\\approx \\frac{f(t) - f(t-\\Delta t)}{\\Delta t}\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t+\\Delta t)\\Delta t \\end{cases} \\] <p>Also known as backward Euler method, because it uses right-hand quadrature method instead of the left-hand rectangle. Implicit method is difficult to evaluate, and usually required iterative methods to solve (e.g. fixed point iteration), but they have better numerical properties and thus more stable.</p>"},{"location":"notes/simulation/numerical-methods/#symplectic-euler-method","title":"Symplectic Euler Method","text":"<p>This one has way too much names, it's also called semi-implicit Euler, semi-explicit Euler, Euler-Cromer, and Newton-St\u00f8rmer-Verlet<sup>1</sup>. </p> \\[ \\begin{cases} f'(t+\\Delta t) &amp;\\approx f'(t) + f''(t)\\Delta t\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t+\\Delta t)\\Delta t \\end{cases} \\] <p>Rewrite it in terms of motion:</p> \\[ \\begin{cases} \\mathbf{v}_{n+1} &amp;\\approx\\mathbf{v}_n + \\mathbf{a}_{n}\\Delta t\\\\ \\mathbf{x}_{n+1} &amp;\\approx\\mathbf{x}_n + \\mathbf{v}_{n+1}\\Delta t\\\\ \\end{cases} \\] \\[ \\begin{align} \\therefore \\mathbf{x}_{n+1}&amp;\\approx\\mathbf{x}_n + (\\mathbf{v}_n + \\mathbf{a}_{n}\\Delta t)\\Delta t\\\\ &amp;\\approx\\mathbf{x}_n + \\mathbf{v}_n\\Delta t + \\mathbf{a}_{n}\\Delta t^2 \\end{align} \\] <p>Look familiar? This is usually seen in game physics to predict a moving body in the next time frame. It is a mix of explicit and implicit method to calculate the final body position. Velocity is integrated explicitly, and position is integrated implicitly by using the velocity from the next time frame. </p>"},{"location":"notes/simulation/numerical-methods/#verlet-method","title":"Verlet Method<sup>2</sup>","text":"<p>This is a second-order numerical method to integrate Newton's equations of motion. </p> <p>We begin by letting the second-derivative \\(\\ddot{\\mathbf{x}}=f''(t)\\) be the change of the finite difference of the backward-difference and forward-difference. In other words, using central difference approximation to find the second derivative. When \\(\\Delta t\\rightarrow 0\\), it will converge to the true second-derivative value. In numerical computations, it will always be an approximation as it's impossible to have an infinitely small step size. </p> <p>Animate</p> \\[ \\begin{align} f''(t) &amp;\\approx \\frac{\\frac{f(t+\\Delta t)-f(t)}{\\Delta t}-\\frac{f(t)-f(t-\\Delta t)}{\\Delta t}}{\\Delta t}\\\\ &amp;\\approx \\frac{\\frac{(f(t+\\Delta t)-f(t))-(f(t)-f(t-\\Delta t))}{\\Delta t}}{\\Delta t}\\\\ &amp;\\approx \\frac{f(t+\\Delta t)-2f(t)+f(t-\\Delta t)}{\\Delta t^2} \\tag{1} \\end{align} \\] <p>It has an interesting property, we can see the approximated second-derivative is independent from its first-derivative. In simpler words, the next position vector \\(\\mathbf{x}_{n+1}\\) can be obtained directly by the previous two \\(\\mathbf{x}_n\\) and \\(\\mathbf{x}_{n-1}\\), without the need of calculating velocity. Continue from \\((1)\\), we get:</p> \\[ \\begin{align} \\ddot{\\mathbf{x}} &amp;\\approx \\frac{\\mathbf{x}_{n+1} - 2\\mathbf{x}_n + \\mathbf{x}_{n-1}}{\\Delta t^2}\\\\ \\ddot{\\mathbf{x}} {\\Delta t^2} &amp;\\approx \\mathbf{x}_{n+1} - 2\\mathbf{x}_n + \\mathbf{x}_{n-1}\\\\ \\mathbf{x}_{n+1}&amp;\\approx2\\mathbf{x}_n - \\mathbf{x}_{n-1} + \\ddot{\\mathbf{x}} \\Delta t^2 \\tag{2} \\end{align} \\]"},{"location":"notes/simulation/numerical-methods/#computing-velocity","title":"Computing Velocity","text":"<p>Because velocities are not explicitly given by definition, it can only be estimated by the mean value theorem.</p> \\[ f'(t) \\approx \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} \\] <p>To erase the dependency or previous position, just shift the time frame by \\(\\Delta t\\) at the cost of accuracy (forward-difference approximation). And you will soon notice it falls back to the form of Euler method.</p> \\[ \\begin{align} f'(t+\\Delta t) &amp;\\approx \\frac{f(t+2\\Delta t) - f(t)}{2\\Delta t}\\\\ &amp;\\approx \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}\\\\ \\end{align} \\] <ol> <li> <p>F.Crivelli. The St\u00f6rmer-Verlet method, 2008. https://www2.math.ethz.ch/education/bachelor/seminars/fs2008/nas/crivelli.pdf \u21a9</p> </li> <li> <p>Gorilla Sun. Euler and Verlet Integration for Particle Physics. https://www.gorillasun.de/blog/euler-and-verlet-integration-for-particle-physics/ \u21a9</p> </li> </ol>"},{"location":"notes/simulation/position-based-fluids/","title":"Position Based Fluids","text":"<p>As discussed in Position Based Simulation, position-based dynamics provided the foundations of simulating lagrangian fluids. Remember PBD is all about how to put constraints on particles and how to resolve them. Similarly, we can model how fluid particles interact by posing some constraints on them.</p>"},{"location":"notes/simulation/position-based-fluids/#density-constraint","title":"Density Constraint","text":"<p>The density constraint<sup>1</sup> aims to model the compressibility on fluids by constraining the density (measuring the amount of mass per unit volume) of a single fluid particle.</p> \\[ C_i(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = \\frac{\\rho_i}{\\rho_o}-1 \\] \\[ \\rho_i = \\sum_j{m_j W(\\mathbf{x}_i - \\mathbf{x}_j, h)} \\] <p>Since we only care about local density around a given particle, the density constraint takes all its neighboring particle positions \\(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}\\), and sum up their masses to \\(\\rho_i\\). Noted that each particles are weighted by a smoothing function \\(W\\) that falls off radially from the center of the kernel, which we will discuss more in the smoothing kernel section. The ratio to its rest density \\(\\rho_o\\) is then evaluated, indicating whether the fluid is being compressed locally (e.g. \\(\\rho_i &gt; \\rho_o\\)). In a perfect equilibrium, \\(\\rho_i\\) should equal to \\(\\rho_o\\) and the constraint \\(C_i=0\\) will be satisfied, meaning this fluid particle is happy :)</p> \\[ C_{density}(\\mathbf{x}) = \\begin{cases} C_1(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = 0\\\\ \\cdots\\\\ C_N(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = 0\\\\ \\end{cases} \\] <p>Scaling up the above to a \\(N\\)-particle simulation, every particles will be assigned a density constraint, to ensure its local density remain the same at all time. This is why the bilateral condition \\(C(\\mathbf{x})=0\\) is used here. Once we run the solver for a decent number of iterations, every fluid particles should be happily resting at their stable positions.</p> <p>What if the density is lower than the rest condition (e.g. \\(\\rho_i &lt; \\rho_o\\))?</p> <p>There will be cases where a lone particle wandered into the void, where a few or even no neighboring particles are present in its surroundings. This posed a tensile instability in our simulation, which can cause clustering or clumping due to negative pressures.</p> <p>This can result in extreme correction vectors and can break the simulation. One solution is to clamp the density constraint result to zero to prevent negative pressure.</p> \\[ C_i(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = \\max(0, \\frac{\\rho_i}{\\rho_o}-1) \\] <p>However, this also means the cohesion nature of fluid particles is gone, particles almost always repel with each other. So by clamping the constraint, we are also limiting ourselves from having this cohesive behavior.</p> <p>Luckily, in the paper \"SPH without a tensile instability\"<sup>2</sup>, an artificial pressure correction factor is proposed. Where \\(\\Delta\\vec{q}\\) is vector to a point at some fixed distance inside the smoothing kernel radius \\(h\\) and \\(k\\) is a small positive constant. \\(\\|\\Delta\\vec{q}\\|=\\{0.1h, \\cdots, 0.3h\\}\\), \\(k = 0.001\\) and \\(n = 4\\) work well in my case.</p> \\[ \\lambda_{corr}=-k\\cdot\\left(\\frac{W(\\vec{r}, h)}{W(\\Delta\\vec{q}, h)}\\right)^n \\]"},{"location":"notes/simulation/position-based-fluids/#smoothing-kernels","title":"Smoothing Kernels","text":"<p>Smoothing kernels are essential for integrating local density of a particle in an SPH simulation. A good kernel needs to fulfill a few requirements:</p> <ul> <li>Radially symmetric (i.e. only depends on distance)</li> <li>Finite support (i.e. particles should only affect up to a certain distance)</li> <li>Differentiable and integrates to 1</li> </ul> Kernel Type Definition Poly6 (2D) \\(W_{poly6}(\\vec{r}, h) = \\frac{4}{\\pi h^8} \\begin{cases} (h^2-\\|r\\|^2)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Poly6 (3D) \\(W_{poly6}(\\vec{r}, h) = \\frac{315}{64 \\pi h^9} \\begin{cases} (h^2-\\|r\\|^2)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (2D) \\(W_{spiky}(\\vec{r}, h) = \\frac{10}{\\pi h^5} \\begin{cases} (h-\\|r\\|)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (3D) \\(W_{spiky}(\\vec{r}, h) = \\frac{15}{\\pi h^6} \\begin{cases} (h-\\|r\\|)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\)"},{"location":"notes/simulation/position-based-fluids/#bloopers","title":"Bloopers","text":"1 2 3 <ol> <li> <p>Macklin, M., &amp; M\u00fcller, M. (2013). Position based fluids. ACM Transactions on Graphics (TOG), 32(4), 1-12.\u00a0\u21a9</p> </li> <li> <p>Monaghan, J. J. (2000). SPH without a tensile instability. Journal of computational physics, 159(2), 290-311.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/simulation/position-based/","title":"Position Based Simulation","text":"<p>As oppose to any other rigidbody simulations, force-based and impulse-based that deals with velocity and acceleration calculations, position-based simulation<sup>1</sup> attempts to minimize the constrains on the position domain down to particle level. </p> <p>This is probably my favorite simulation technique!! It's extendable, reuseable, and can simulate all the things. Soft body, fluid, semi-rigidbody, you name it!</p>"},{"location":"notes/simulation/position-based/#problem","title":"Problem","text":"<p>Given a set of \\(M\\) constrains (basically means there are \\(M\\) equality or inequality equations to satisfy), we need to solve for \\(N\\times\\mathbb{R}^3\\) unknowns to resolve our final positions. Most of the time, the number of constraints won't match the number of unknowns we are solving (i.e. \\(M \\neq N\\)). This means if the problem was a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), the solution can't be obtained easily by inverting the matrix and solve for \\(\\mathbf{x}\\). Not to mention the system we are solving won't necessarily be linear. For example, a simple distance constraint \\(C_i(\\mathbf{x}_1, \\mathbf{x}_2)=\\left|\\mathbf{x}_1-\\mathbf{x}_2\\right|^2-d^2\\) alone is a non-linear equation.</p> <p>Thus, it all boils down to a problem of finding a set of positions \\(\\mathbf{x}\\) that minimize if not solved the constrained system:</p> \\[ C(\\mathbf{x}) = \\begin{cases} C_1(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N) \\succ 0\\\\ \\cdots\\\\ C_M(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N) \\succ 0\\\\ \\end{cases} \\] <p>Where \\(\\mathbf{x}\\) is the concatenation of \\(N\\times\\mathbb{R}^3\\) positions we are trying to solve, and the symbol \\(\\succ\\) denotes either \\(=\\) or \\(\\geq\\). \\(C(\\mathbf{x})=0\\) means the constraint has a bilateral condition enforced, usually representing strong forces which should always hold true. On the other hand, \\(C(\\mathbf{x}) \\geq 0\\) or \\(C(\\mathbf{x}) \\leq 0\\) means the constraint is loosely enforced. They are usually seen in collision constraints where particles are free to move on one side, but preventing them to enter the other side. </p>"},{"location":"notes/simulation/position-based/#non-linear-gauss-seidel-solver","title":"Non-Linear Gauss-Seidel Solver","text":"<p>As said in the previous section, there won't be a closed-form solution because the system is neither symmetric nor linear. Our best bet is to apply an iterative solver to minimize the system after a fixed amount of iterations and hope for the best that the approximated result will satisfy all our constraints. This is where the non-linear Gauss-Seidel algorithm comes in.</p> <p>Since all constraints in the system can't be solved at once, then each constraint equations need to be solved separately. At the end of each iteration, each particles should have a correction vector \\(\\Delta\\mathbf{x}\\) such that most constraints should reach their condition \\(C_i(\\mathbf{x}+\\Delta\\mathbf{x})\\succ0\\). Thus we arrived to this constraint equation:</p> \\[ C(\\mathbf{x}+\\Delta\\mathbf{x})\\approx C(\\mathbf{x})+\\nabla C(\\mathbf{x})\\cdot\\Delta \\mathbf{x} \\succ 0 \\tag{1} \\] <p>Here \\(\\nabla C(\\mathbf{x})\\) is the gradient of given constraints, and the equation above literally means stepping from the current constraint value \\(C(\\mathbf{x})\\) with a step size \\(\\Delta\\mathbf{x}\\) should be ended up equal or greater or equal than 0. The paper mentioned that by restricting the step direction to be \\(\\nabla C(\\mathbf{x})\\), satisfies the requirement for linear and angular momentum conservation. It also means only Lagrange multiplier scalar \\(\\lambda\\) has to be found to solve the correction equation \\((1)\\).I can't comprehend this part</p> <p>The position correction vector \\(\\Delta\\mathbf{x}\\) will then be evaluated as:</p> \\[ \\Delta\\mathbf{x}=\\lambda \\mathbf{M}^{-1}\\nabla C(\\mathbf{x}) \\] <p>where \\(\\mathbf{M}=diag(m_1, m_2, \\cdots, m_N)\\) represents the mass of each particles. So the correction vector of particle \\(i\\) will be:</p> \\[ \\begin{cases} \\Delta\\mathbf{x}_i &amp;= -\\lambda_i w_i \\nabla_{\\mathbf{x}_i} C_i(\\mathbf{x})\\\\ \\lambda_i &amp;= \\frac{C_i(\\mathbf{x})}{\\sum_j{w_j\\left|\\nabla_{\\mathbf{x}_j}C_i(\\mathbf{x})\\right|^2}} \\end{cases} \\]"},{"location":"notes/simulation/position-based/#algorithm","title":"Algorithm","text":"<p>The main position based dynamics (PBD) algorithm can be split into three different stages: prediction, solving constraints, and post-solve updates.</p> \\[ \\begin{align} &amp;\\mathbf{while}\\ \\text{simulating}\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow \\mathbf{v}_i + \\mathbf{f}_{ext}\\Delta t\\\\ &amp;\\quad\\quad \\mathbf{p}_i \\leftarrow \\mathbf{x}_i + \\mathbf{v}_i\\Delta t\\\\ &amp;\\quad \\mathbf{for} \\text{ all constraints } C\\\\ &amp;\\quad\\quad solve(C, \\Delta t)\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow (\\mathbf{x}_i - \\mathbf{p}_i) \\frac{1}{\\Delta t}\\\\ &amp;\\quad\\quad \\mathbf{x}_i \\leftarrow \\mathbf{p}_i \\end{align} \\] <p>Noted that this non-linear constrained system is \"solved\" by iterating each constraints and optimizing them sequentially, which to me, doesn't really \"solve\" the system but rather find a close-enough solution. This is also why it's plausible that a particle will end up violating one of the constaints (possibly clipping through geometries) after the solver reached its maximum iterations. </p>"},{"location":"notes/simulation/position-based/#result","title":"Result","text":"<ol> <li> <p>Jan Bender, Mattias M\u00fcller, Miles Macklin. Position-Based Simulation Methods in Computer Graphics, Eurographics 2015. http://mmacklin.com/EG2015PBD.pdf \u21a9</p> </li> </ol>"},{"location":"notes/software-dev/ui-library/","title":"Prelude","text":"<p>I know I know, this is probably a waste of time to reinvent the wheel. There are plenty GUI libraries out in the wild that could save me tons of time without reinventing it. But my past experience tells me that these libraries are usually quite heavy, and dependencies mean I will have to wait for fixes/patches if anything was broken. Having all that frustration makes me think, even it's annoying to build a UI library from scratch, maybe it's worth making it on my own. Perhaps I can learn something, or ended up having the same struggle the community have, who knows? It will be a valueable experience nonetheless.</p> <p>In my own toy engine, I want to make retained mode and immediate mode GUI work together because I see merits in both techniques. For punctual widgets (part of the core engine), I will certainly make it in retained mode. But for utilities and debug tools, I want it to be prototyped in immediate mode first. Then if it ever got mature enough to be a standalone widget, it will certainly be rewritten in retained mode. At least this is how I think about the long-term development. </p>"},{"location":"notes/software-dev/ui-library/immediate/","title":"Immediate Mode GUI","text":"<p>I am making my own ImGui library to learn a bit how the mainstream Dear ImGui library actually works. The inner workings of Dear ImGui wasn't documented or explained anywhere else, so I might just do a shallow investigation into their code base and breifly talk about how it inspires my own implementation.</p>"},{"location":"notes/software-dev/ui-library/immediate/#background","title":"Background","text":"<p>Immediate Mode GUI was initially suggested by Casey Muratori<sup>1</sup> back in 2005. It's often being compared to the traditional retained mode GUI, e.g. Qt. ImGui on the other hand is declarative, using logical flow in the program to determine what to draw in a frame. This enables faster prototyping and writing throw-away code without any hassle. </p>"},{"location":"notes/software-dev/ui-library/immediate/#_1","title":"Immediate Mode GUI","text":""},{"location":"notes/software-dev/ui-library/immediate/#state-storage","title":"State Storage","text":"<p>Since immediate mode means everything are declarative, there will be no states recorded. So how are we suppose to have states shared across frames? The answer is of course, putting them in a storage that is managed by the context object. </p> <p>For example, an expandable element is clicked open on frame 1. Before we draw the expandable at frame 2, it needs to query the storage about the state of the expandable in order to determine whether to draw its expanded contents or not. But how to write the state of the expandable?</p>"},{"location":"notes/software-dev/ui-library/immediate/#element-id","title":"Element ID","text":"<p>The state storage itself shouldn't be complicated. It can be just a hash map to a state struct. The problem is how to address immediate mode UI elements? They are not guaranteed to be consistent across frames. Meaning we cannot give each elements with an incremental counter.</p> <p>Luckily, the element stack stays relatively stable across frames. By saying \"stack\", they are usually pushed with <code>Begin*()</code> and poped with <code>End*()</code> (or manually by <code>PushID()</code> and <code>PopID()</code>). This essentially wraps a \"namespace\" around the stack elements, such that duplicated names are allowed. For example:</p> <pre><code>\u2022 label1\n&gt; myExpandable\n  \u2022 label2\nv folder\n  \u2022 label3\n  &gt; myExpandable\n</code></pre> <p>Here, the first <code>myExpandable</code> at root level has a hash value already. To prevent collision, the second level <code>myExpandable</code> will have its hash seeded by its parent <code>folder</code>'s hash. Now the name collision issue should be resolved. In case of inevitable collisions, Dear ImGui solves this by allowing custom IDs with <code>&lt;name&gt;###&lt;customID&gt;</code> to override the hashing input. </p> <p>And as a record, Dear ImGui chose CRC32 as the hashing algorithm. Although there's a FIXME in the code saying using FNV1a can potentially speed up the hashing process as there's no need to do random access to a look up table, proper measurements are needed to conclude its effectiveness.</p>"},{"location":"notes/software-dev/ui-library/immediate/#clip-rect","title":"Clip Rect","text":"<p>There's a term called \"Clip Rect\" in Dear ImGui, mainly used to prevent child elements being drawn outside the window. This can be easily achieved by scissor test in the rasterize pipeline. For complex shapes, stencil test is required, but it's an overkill for simple axis-aligned UI elements. For now, a simple scissor test should be sufficient. </p> <p>Calling <code>PushClipRect()</code> and <code>PopClipRect()</code> will push a rect to the stack, then the renderer will set the topmost clipping region as the scissor testing region. This can effectively discard any pixels that exceeded the boundary.</p>"},{"location":"notes/software-dev/ui-library/immediate/#cursors","title":"Cursors","text":"<p>As anybody knows about cursors, there are two types, hardware and software. Hardware cursor movement wasn't limited by the frame rate of the application, usually it's done by calling platform specific functions (like <code>SetCursor</code> in Windows, and <code>NSCursor</code> in OSX). Dear ImGui appears to use the corresponding API provided by the backends, which is mainly hardware/OS cursors.</p>"},{"location":"notes/software-dev/ui-library/immediate/#my-version","title":"My Version","text":"<p>As I said in the Prelude section, my goal is to write minimal code to support both retained and immediate mode UI elements.  // TODO: Write more when I'm confident and consolidated the current architecture design.</p> <ol> <li> <p>Casey Muratori (2005). Immediate-Mode Graphical User Interfaces. https://caseymuratori.com/blog_0001\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/software-dev/ui-library/retained/","title":"Retained Mode GUI","text":""},{"location":"notes/software-dev/ui-library/retained/#ui-placement","title":"UI Placement","text":"<p>I did a lot of references on how other game engines handle graphical user interfaces, i.e. Unreal, Unity, Godot, etc. They all seem to arrive to a consensus on how to do dynamic UI positioning. That is, using anchor and offsets.  </p>"},{"location":"notes/software-dev/ui-library/retained/#anchor","title":"Anchor","text":"<p>Anchor defines a rectangular area relative to the parent element in terms of its ratio. On the x-axis, 0 represents the left edge and 1 represents the right edge. On the y-axis, o represents the top edge and 1 represents the bottom edge. If anchor is set to (left: 0.1, top: 0.1, right: 0.8, bottom: 0.8), this means the child element will start at 10% until 80% of the parent bound. </p>"},{"location":"notes/software-dev/ui-library/retained/#offset","title":"Offset","text":"<p>Offset is literally the offsets from the rectangular area defined by the anchor attribute as shown above. Following the same coordaintes, +X points to the right of the screen and +Y points to the bottom of the screen. This is just a way to fine tune the position of the child UI element relative to its parent.</p>"},{"location":"notes/software-dev/ui-library/retained/#adding-it-up","title":"Adding it up!","text":"<p>To make it more obvious, here's a code snippet describing how the child region is calculated from its parent bound.</p> <pre><code>UIRect {\n  .left = parent_rect.left + parent_width * anchor.left + offset.left,\n  .top = parent_rect.top + parent_height * anchor.top + offset.top,\n  .right = parent_rect.left + parent_width * anchor.right + offset.right,\n  .bottom = parent_rect.top + parent_height * anchor.bottom + offset.bottom\n}\n</code></pre>"}]}