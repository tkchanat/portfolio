{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Andy (Tsz Kin) Chan <p> tkchanat |  @tkchanat |  Andy Chan |  Vancouver, BC</p> <p></p> <p>I am a rendering software engineer at Netflix Animation Studios Vancouver, developing in-house production renderer Glimpse. I love both offline and realtime graphics. My main interests are light transport and Monte Carlo path tracer, but any compute graphics related topics fascinate me too.</p> <p>This site is my dedicated graphics dump yard where you can (occasionally) find some useful knowledge. Happy diving!</p>"},{"location":"#experience","title":"Experience","text":"<p><sub>* Trademarks of the above are owned by their respective companies and publishers.</sub></p>"},{"location":"#publication","title":"Publication","text":"<p>Can You See the Heat? A Null-scattering Approach for Refractive Volume Rendering - SIGGRAPH 2023</p> <p>Basile Fraboni, Tsz Kin Chan, Thibault Vergne, Jakub Jeziorski</p> <p>[Project Page] | [Paper]</p>"},{"location":"#personal-projects","title":"Personal Projects","text":"<p>Migrating from old webpage </p>"},{"location":"#education","title":"Education","text":"<p>Hong Kong University of Science and Technology, HK (2016 - 2020)</p> <p>Bachelor of Engineering, Computer Science</p> <ul> <li>Advanced Computer Graphics</li> <li>Applied Statistics and Linear Algebra</li> <li>Computer Organization and Operating System</li> <li>Data Structure and Algorithms</li> <li>Software Engineering</li> </ul> <p>KTH Royal Institue of Technology, Sweden (2019 Jan - June)</p> <p>Exchange Study, Master's Program</p> <ul> <li>Computer Graphics and Interaction</li> <li>Music Communication and Music Technology</li> <li>Machine Learning</li> </ul>"},{"location":"#skills","title":"Skills","text":"Tools &amp; Application VSCode VisualStudio Git OpenUSD Unity UE4 Blender Programming Languages C++ Rust Python C# Javascript Graphics API Vulkan OpenGL DirectX 11 WebGL wgpu"},{"location":"notes/microfacet/","title":"Microfacet Theory","text":"<p>There is a well-written paper by Eric Heitz<sup>1</sup> explaining every details about the microfacet theory. I highly recommend everyone to read it to get a thorough understanding from beginning to end. The stuff I write here was just my rough understanding of the work, happy to correct this page anytime. </p>"},{"location":"notes/microfacet/#radiance-on-surface","title":"Radiance on Surface","text":"<p>Here is the definition of radiance from wikipedia.</p> <p>In radiometry, radiance is the radiant flux emitted, reflected, transmitted or received by a given surface, per unit solid angle per unit projected area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>It matches the one defined in Veach's thesis, which is written as:</p> \\[ \\begin{align*} L(\\omega, \\mathbf{x}) &amp;= \\frac{d^2\\Phi(\\omega)}{dA^\\bot_{\\omega}(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{dA(\\mathbf{x})\\ d\\sigma^\\bot_{\\omega}(\\omega)} \\end{align*} \\] <p>The \\(A^\\bot_{\\omega}\\) here means the area projected from the hypothetical surface onto the observed direction \\(\\omega\\). For a real surface, the projected area measure \\(dA^\\bot_{\\omega}\\) in a real surface can be written as \\(\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\). Even better, the dot product \\(\\left|\\omega\\cdot \\mathbf{n}\\right|\\) can be transfered into the solid angle measure \\((\\sigma_\\omega \\rightarrow \\sigma^\\bot_{\\omega})\\), which can be beneficial sometimes if we want to leverage that property during integration, e.g. Stratified sampling of projected spherical caps, EGSR 2018.</p> <p>Extending this to a microfacet model, where every micronormals \\(\\omega_m\\) has its own direction, we need to integrate that over a small patch of surface \\(\\mathcal{M}\\). </p> \\[ L(\\omega_o, \\mathcal{M}) = \\frac{\\int_\\mathcal{M}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{M})} \\] <p>For every infinitesimal points \\(\\mathbf{x}\\in\\mathcal{M}\\), its local radiance towards the observing direction is \\(A^\\bot_{\\omega_o}L(\\omega_o,\\mathbf{x})\\). So the integrated result will be the total amount of watts per steradian \\([W\\cdot sr^{-1}]\\) that is radiating off to the observing direction \\(\\omega_o\\). The purpose of the denominator \\(A^\\bot_{\\omega_o}(\\mathcal{M})\\) which represents the total projected area of \\(\\mathcal{M},\\) is here merely to normalize the entire thing back to per unit area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>Fun little fact, for marcosurface \\(\\mathcal{G}\\) which its normal doesn't vary within the domain, it's radiance is no difference than the original definition of radiance.</p> \\[ \\begin{align*} L(\\omega_o, \\mathcal{G}) &amp;= \\frac{\\int_\\mathcal{G}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= \\frac{A^\\bot_{\\omega_o}(\\mathcal{G})\\ L(\\omega_o, \\mathbf{x})}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= L(\\omega_o, \\mathbf{x}) \\end{align*} \\]"},{"location":"notes/microfacet/#normal-distribution-function","title":"Normal Distribution Function","text":"<p>Spatial integral isn't ideal to work with because the whole microfacet theory is based on a statistical model, it's not necessarily tied to an actual spatial representation. That being said, we need a way to convert that integral into the solid angle domain. Luckily, the distribution of normals is able to provide just that.</p> \\[ D(\\omega) = \\int_\\mathcal{M}{\\delta_\\omega(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x}} \\] <p>It is expressed in square meters per steradian \\([m^2\\cdot sr^{-1}]\\), representing the density of normals that aligns a given direction \\(\\omega\\) over the surface \\(\\mathcal{M}\\), hence the dirac delta function \\(\\delta_\\omega\\). To better understand this magical function, imagine counting the number of micronormals in the patch that are pointing to the direction \\(\\omega\\) within the domain space \\(\\mathcal{M}\\). Its sum will be the area covering the selected population of normals, and this is where the area term \\(m^2\\) comes from. This adds on top of the fact that dirac delta always has the inverse dimension of its argument, which is in \\(sr\\) here, so its unit is per steradian \\(sr^{-1}\\). </p> <p>Let's talk about an interesting property of the integral of normal distribution. Over any arbitrary solid angle region \\(\\Omega'\\subset\\Omega\\) on the unit sphere, it always gives the total covered area whose normals lie in \\(\\Omega'\\).</p> \\[ \\int_\\mathcal{M'}1\\ d\\mathbf{x} = \\int_{\\Omega'} D(\\omega_m)\\ d\\omega_m \\] <p>This makes \\(\\int_\\Omega D(\\omega_m)\\ d\\omega_m\\) the total area of the microsurface \\(\\mathcal{M}\\). Magical, right? Now any attempts of integrating a function of the microsurface normal \\(\\omega_m\\) spatially over \\(\\mathcal{M}\\) can be converted into a statistial integral:</p> \\[ \\int_\\mathcal{M}f(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x} = \\int_\\Omega f(\\omega_m)\\ D(\\omega_m)\\ d\\omega_m \\] <p>TODO...</p>"},{"location":"notes/microfacet/#geometric-attenuation","title":"Geometric Attenuation","text":"<p>\\(G_1(\\omega_o, \\omega_m)\\) is the statistical masking function of micro-normal \\(\\omega_m\\). </p> <p>\\(G_1(\\omega_i, \\omega_m)\\) is the statistical shadowing function of micro-normal \\(\\omega_m\\)</p> <p>\\(G(\\omega_i, \\omega_o)\\) is the shadowing-masking term</p>"},{"location":"notes/microfacet/#various-models","title":"Various Models","text":"<p>Notation</p> <p>\\(\\chi^+(a)\\) is a special step function, called Heaviside function. It is defined as \\(1\\) if \\(a&gt;0\\) and \\(0\\) if \\(a\\leq 0\\).</p> <p>\\(\\omega_m\\) is my preferred notation for micro-normal, it's also known as \\(\\mathbf{h}\\) or \\(\\mathbf{m}\\).</p> <p>\\(\\omega_g\\) is my preferred notation for geometric normal, it is also known as \\(\\mathbf{n}\\) or \\(Z=(0,0,1)\\) if in local tangent-space.</p>"},{"location":"notes/microfacet/#trowbridgereitz-ggx","title":"Trowbridge\u2013Reitz (GGX)","text":"<p>There are way too many forms online, I picked the known ones to show you that they are all equivalent. </p> \\[ \\begin{align} D(\\omega_m, \\alpha) &amp;= \\frac{\\alpha^2\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi cos^4\\theta_m(\\alpha^2+tan^2\\theta_m)^2} &amp; \\text{Walter et al. 2007} \\\\ &amp;= \\frac{\\alpha^2\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi cos^4\\theta_m (\\frac{sin^2\\theta_m}{cos^2\\theta_m}+\\alpha^2\\frac{\\cancel{cos^2\\theta_m}}{\\cancel{cos^2\\theta_m}})^2} \\\\ &amp;= \\frac{\\alpha^2\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi (sin^2\\theta_m+\\alpha^2cos^2\\theta_m)^2} \\\\ &amp;= \\frac{\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi \\alpha^2 (\\frac{1}{\\alpha^2}(sin^2\\theta_m+\\alpha^2cos^2\\theta_m))^2} \\\\ &amp;= \\frac{\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi \\alpha^2 (\\frac{sin^2\\theta_m}{\\alpha^2}+cos^2\\theta_m)^2} \\\\ &amp;= \\frac{\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi \\alpha^2 (\\frac{x_m^2 + y_m^2}{\\alpha^2}+z_m^2)^2} \\\\ D(\\omega_m, \\alpha_x, \\alpha_y) &amp;= \\frac{\\chi^+(\\omega_m\\cdot\\mathbf{\\omega_g})}      {\\pi \\alpha_x \\alpha_y (\\frac{x_m^2}{\\alpha_x^2}+\\frac{y_m^2}{\\alpha_y^2}+z_m^2)^2} &amp; \\text{Heitz 2018} \\end{align} \\] \\[G(\\mathbf{l},\\mathbf{v},\\omega_m)=\\chi^+\\left(\\frac{\\left&lt;\\mathbf{v}\\cdot\\omega_m\\right&gt;}{\\left&lt;\\mathbf{v}\\cdot\\mathbf{n}\\right&gt;}\\right)\\quad\\frac{2}{1+\\sqrt{1+\\alpha^2tan^2\\theta_\\mathbf{v}}}\\]"},{"location":"notes/microfacet/#unreal-4-approximation","title":"Unreal 4 Approximation","text":"<p>Epic Games<sup>4</sup> mostly adopts the formulations of \\(F\\) and \\(G\\) from the Schlick's model with slight modifications, but picked the \\(D\\) in the Disney's GGX model. These are obviously an approximation on top of an approximation. Every decisions made here are sacrificing minor visual error for faster computations.</p> \\[F(\\mathbf{v},\\omega_m)=f_0+(1-f_0)\\cdot2^{(-5.55473\\left&lt;\\mathbf{v}\\cdot\\omega_m\\right&gt;-6.98316)\\left&lt;\\mathbf{v}\\cdot\\omega_m\\right&gt;}\\] \\[D(\\omega_m)=\\frac{\\alpha^2}{\\pi(\\left&lt;\\mathbf{n}\\cdot\\omega_m\\right&gt;^2(\\alpha^2-1)+1)^2}\\] \\[G(\\mathbf{l},\\mathbf{v},\\omega_m)=G_1(\\mathbf{l})\\ G_1(\\mathbf{v}),\\quad G_1(\\mathbf{v})=\\frac{\\left&lt;\\mathbf{n}\\cdot\\omega_m\\right&gt;}{\\left&lt;\\mathbf{n}\\cdot\\omega_m\\right&gt;(1-k)+k},\\quad k=\\frac{(\\alpha+1)^2}{8}\\] <ol> <li> <p>Heitz E. (2014). Understanding the masking-shadowing function in microfacet-based BRDFs. Journal of Computer Graphics Techniques, 3(2), 32-91. \u21a9</p> </li> <li> <p>Walter B., Marschner S. R., Li H., &amp; Torrance K. E. (2007, June). Microfacet models for refraction through rough surfaces. In Proceedings of the 18th Eurographics conference on Rendering Techniques (pp. 195-206). \u21a9</p> </li> <li> <p>Schlick C. (1994, August). An inexpensive BRDF model for physically\u2010based rendering. In Computer graphics forum (Vol. 13, No. 3, pp. 233-246). Edinburgh, UK: Blackwell Science Ltd. \u21a9</p> </li> <li> <p>Karis B, Epic Games. (2013). Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 4(3), 1. \u21a9</p> </li> </ol>"},{"location":"notes/monte-carlo-integration/","title":"Monte Carlo Integration","text":""},{"location":"notes/monte-carlo-integration/#motivation","title":"Motivation","text":""},{"location":"notes/monte-carlo-integration/#quadrature-rules","title":"Quadrature rules","text":"<p>To integrate the function \\(I = \\int_{\\Omega}{f(x) dx}\\), where \\(\\Omega\\) is the domain of integration. If \\(f(x)\\) known to you and can be evaluated analytically, then you are good. But usually that's not the case, \\(f(x)\\) behaves like a blackbox (you give it an input and it spits out an output), it needs to be integrated numerically. </p> <p>Pretend you don't know the equation for the following function and you want to know its area bounded by it. In high school, we were taught that the integral can be approximated with infinitesimal rectangles, a.k.a. the rectangle rules. So let's put together a bunch of rectangles and crunch some numbers!</p> <p>As the number of rectangles grows, the closer it fills the target area. To which I can say with confidence the area is \\(\\pi/4\\) since it's obviously a quarter of a circle. </p> <p>There are more similar approaches to do integration and they are known as quatrature rules. The generalized form looks like this \\(\\hat{I} = \\sum_{i=1}^n w_i\\ f(x_i)\\). The weight \\(w_i\\) is defined differently in other rules, e.g. Midpoint rule, Trapezoid rule, Simpson's rule, etc. </p>"},{"location":"notes/monte-carlo-integration/#downside","title":"Downside","text":"<p>These kind of numerical approximations suffers from two major problems. </p>"},{"location":"notes/monte-carlo-integration/#high-frequency-signals","title":"High-frequency signals","text":"<p> Graphs like this can't be easily approximated with thick bars. To get closer to the real value, the bars must be really narrow which means more iterations to compute. </p>"},{"location":"notes/monte-carlo-integration/#high-dimensional-domains","title":"High-dimensional domains","text":"\\[ \\hat{I}=\\sum_{i_1=1}^n \\sum_{i_2=1}^n \\dots \\sum_{i_s=1}^n{w_{i_1} w_{i_2} \\dots w_{i_s} f(x_{i_1}, x_{i_2}, \\dots, x_{i_s})} \\] <p>where \\(s\\) is the dimension, \\(w_i\\) is the weights and \\(x_i\\) is the sample position in the domain. Which is always the case when solving light transport problems.</p>"},{"location":"notes/monte-carlo-integration/#definition","title":"Definition","text":"<p>As the name suggests, we can integrate functions with stochastic approaches through random sampling. With the Monte Carlo method, we can tap into the power of integrating arbitrary multi-dimensional functions!</p> \\[ F_N = \\frac{1}{N} \\sum_{i=1}^N{ \\frac{f(X_i)}{p(X_i)} } \\] <p>where \\(p(X_i)\\) is the probability density function (pdf). And we can verify that with sufficient samples, it will eventually converge to the expected value \\(I\\).</p> \\[ \\begin{align} E[F_N] &amp;= E[\\frac{1}{N}\\sum_{i=1}^N{\\frac{f(X_i)}{p(X_i)}}] \\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^N{\\int_{\\Omega}{\\frac{f(x)}{p(x)} p(x) d\\mu(x)}} \\\\ &amp;= \\int_{\\Omega}{f(x) d\\mu(x)} \\\\ &amp;= I \\end{align} \\] <p>It comes with the following benefits:</p> <ol> <li>It has a convergence rate of \\(O(\\frac{1}{\\sqrt{N}})\\) in any number of dimensions, which quadrature rule methods cannot achieve.</li> <li>Simplicity. All you need is two functions <code>sample()</code> and <code>eval()</code>, and occationally finding a suitable pdf.</li> </ol>"},{"location":"notes/monte-carlo-integration/#sampling-random-variables","title":"Sampling random variables","text":"<p>Generating uniform samples (e.g. \\(p(x)\\) is a constant) gurantees convergence but its rate is much slower. Ultimately, the samples should be drawn from a specific distribution such that most contribution to the integral is being extracted quickly from the domain. In other words, sampling carefully reduces the time to compute the ground-truth result.</p>"},{"location":"notes/monte-carlo-integration/#inverse-transform-sampling","title":"Inverse Transform Sampling","text":"<p>This is a method for generating random variables from a given probability distribution (pdf) by using its inverse cumulative distribution (cdf). Imagine the likelihood of picking a random variable \\(X\\) follows a normal distribution, and we want the samples to be drawn proportional to its likeliness. A cdf table is built by summing up the marginal distributions (It's totally fine that the pdf doesn't add up to 1, since the sample will be drawn from the range of cdf which can be normalized by its maximum value). Then uniform samples \\(U\\) are drawn in the inverse domain of cdf such that random variable \\(X\\) is picked with \\(X = cdf^{-1}(U)\\).</p> <p>Add 1 sample Add 10 samples Reset</p> <p>From the above example, we can see most samples are centered at the highest probability area while the tails on both sides will have lower sample count. Good thing with this method is that it can be easily extended to multi-dimensional cases, and stratifying samples in the domain helps improves exploring the entire domain. </p>"},{"location":"notes/monte-carlo-integration/#downside_1","title":"Downside","text":"<p>The pdf must be known, also building the cdf takes time and memory. And if the cdf cannot be inverted analytically, numerically computing the inverse mapping value (e.g. binary search) on every drawn samples is quite costly.</p> <p>Note: A more efficient sampling structure exists out there, and it's called the Alias Method, which samples can be drawn in constant \\(O(1)\\) time. // TODO: Make a page about this</p>"},{"location":"notes/monte-carlo-integration/#rejection-sampling","title":"Rejection Sampling","text":"<p>When the pdf is difficult to sample, we can instead sample from a simpler density \\(q\\) such that \\(p(x) \\le M q(x)\\), where \\(M\\) is a scaling constant. For instance, you want to integrate the area of an arbitrary shape but directly sampling the shape is hard. However, you know it's much easiler to draw samples from a box. So let's throw some dots onto our sample space!</p> <p>Start Reset</p> <p>The sampling space is defined as the tight bounding box of the shape, since drawing samples from a square is much simpler. We know the probability \\(p\\) of picking a sample point inside the shape is always less than the density \\(q\\) (it's completely enclosed within the bound), it's eligible to use this strategy to draw a sample. To draw one:</p> <ol> <li>Sample \\(X_i\\) according to \\(q\\) (draw a point inside the square)</li> <li>Sample \\(U_i\\) uniformly on \\([0, 1]\\)</li> <li>If \\(U_i \\le p(X_i) / (Mq(X_i))\\), return the sample \\(X_i\\)</li> <li>Else, repeat 1</li> </ol> <p>In the above case, \\(p(X_i)\\) is \\(\\frac{1}{area}\\) when the point is drawn inside the shape else zero, and \\(q(X_i)\\) is always \\(\\frac{1}{64}\\) because we are uniformly sampling a 8x8 square. Given that \\(U_i\\) has a trivial probability of being 0, we can safely assume that all valid samples \\(X_i\\) are located inside the shape. Thus we know they are good samples.</p>"},{"location":"notes/monte-carlo-integration/#importance-sampling","title":"Importance Sampling","text":"<p>\\(N=\\) \\(\\int{f(x)}dx =\\) Approx \\(=\\) Start Reset</p>"},{"location":"notes/monte-carlo-integration/#bsdf-sampling","title":"BSDF Sampling","text":"\\[ p(w_i') \\propto f_s(\\mathbf{x}', w_i' \\rightarrow w_o') \\] <p>Depending on the surface properties, there are certain directions (\\(w_i'\\)) the BSDF favors after an interaction. To get the more contribution from the function, samples have to be drawn proportional to the \\(f_s\\)'s shape. Because BSDF samples are drawn inside the solid angle domain, the probability \\(p(w_i')\\) is also measured in solid angle.</p>"},{"location":"notes/monte-carlo-integration/#light-sampling","title":"Light Sampling","text":"<p>In case when BSDF failed to find a significant contribution, other words the outgoing ray direction missed the light source, light sampling then comes into play to provide as a backup strategy.</p> \\[ L_o(\\mathbf{x}'\\rightarrow\\mathbf{x}'') = \\int_{\\mathcal{M}}{f_s(\\mathbf{x}\\rightarrow\\mathbf{x}'\\rightarrow\\mathbf{x}'')L_e(\\mathbf{x}\\rightarrow\\mathbf{x}')G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')dA(\\mathbf{x})} \\] <p>\\(G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\) often refers as the geometric term, which was introduced because of change of variables. When changing from projected solid angle \\(d\\sigma^\\bot(w_i')\\) to area measure \\(dA(\\mathbf{x})\\), it is required to have this term to normalize the integral:</p> \\[ G(\\mathbf{x}\\leftrightarrow\\mathbf{x}') = V(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\frac{cos(\\theta_o)cos(\\theta_i')}{||\\mathbf{x}-\\mathbf{x}'||^2} \\] <p>To importance sample the light instead of solid angle, the density \\(p(\\mathbf{x})\\) is predetermined since it is just the probability of picking a point on the manifold surface, i.e. \\(p(\\mathbf{x})=\\frac{1}{Area}\\). </p>"},{"location":"notes/monte-carlo-integration/#transformation-of-space-conversion-of-domain","title":"Transformation of Space / Conversion of Domain","text":"\\[ p(\\mathbf{x})=p(w_i')\\frac{d\\sigma^\\bot(w_i')}{dA(\\mathbf{x})}=p(w_i')\\frac{|cos(\\theta_o)cos(\\theta_i')|}{||\\mathbf{x}-\\mathbf{x}'||^2} \\]"},{"location":"notes/monte-carlo-integration/#multiple-importance-sampling","title":"Multiple Importance Sampling","text":"<p>Next event estimation can be seen as a multiple importance sampling<sup>1</sup> approach of integrating the radiance since it combines two sampling strategies, BSDF sampling and light sampling, often called as direct and indirect lighting. </p> <ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/sampling-techniques/","title":"Sampling Techniques","text":"<p>Puseudo-random noise usually don't produce good uniform samples. It often produces clusters and void within the sample space. That means the sample space is not well explored, either wasting samples on similar areas or even complete ignoring some subregions. And this scales to any dimensions.</p> <p></p>"},{"location":"notes/sampling-techniques/#stratified-sampling","title":"Stratified Sampling","text":"<p>Instead of recklessly scatter points around, how about subdiving the domain \\(\\Omega\\) into non-overlapping regions \\(\\Omega_1, \\dots, \\Omega_n\\). Noted that the union of them must cover the whole domain.</p> \\[ \\bigcup_{i=1}^n{\\ \\Omega_i} = \\Omega \\] <p></p> <p>This works pretty well in low-dimensional integration problems. However unfortunately, this suffers the same problem as the quadrature rules where it won't perform well in high frequency signals.</p>"},{"location":"notes/sampling-techniques/#low-discrepancy-sequences","title":"Low-Discrepancy Sequences","text":"<p>If randomness are difficult to control, how about herding the points to position in a deterministic pattern such that they are almost equal distance to each other? In other words, how do we get an equidistribution of samples?</p>"},{"location":"notes/sampling-techniques/#defining-discrepancy","title":"Defining Discrepancy","text":"<p>Imagine a \\(s\\)-dimentional unit cube \\(\\mathbb{I}^s = [0, 1)^s\\), with a point set \\(P = {x_1, x_2, \\dots, x_N} \\in \\mathbb{I}^s\\). We define the point set discrepancy<sup>1</sup> \\(\\mathcal{D}(J, P)\\) as follows:</p> \\[ \\mathcal{D}(J, P) = \\left|\\frac{A(J)}{N} - V(J)\\right| \\] <p>You can think of \\(\\mathcal{D}(J, P)\\) as the proportion of points inside a sub-interval \\(J\\), where \\(A(J)\\) is the number of points \\(x_i \\in J\\) and \\(V(J)\\) is the volume of \\(J\\). </p> <p>The worst-case discrepancy is called the star-discrepancy and is defined as:</p> \\[ \\mathcal{D^*}(N)=\\sup_{J\\in\\mathbb{I^s}}{|\\mathcal{D}(J;P)|} \\] <p>A good sequence should minimize such star-discrepancy \\(\\mathcal{D^*}(N)\\) to be qualified as low-discrepancy. Perhaps it's easier to visualize the terms in a diagram.</p> <p>\\(N=\\) \\(\\mathcal{D}(A, P) =|\\)\\(\\frac{A(A)}{N}-V(A)\\)\\(|=\\) \\(\\mathcal{D}(B, P) =|\\)\\(\\frac{A(B)}{N}-V(B)\\)\\(|=\\) \\(\\mathcal{D}(C, P) =|\\)\\(\\frac{A(C)}{N}-V(C)\\)\\(|=\\) \\(\\mathcal{D^*}(P) =\\)</p> <p>Generate Point Set Reset</p>"},{"location":"notes/sampling-techniques/#halton-sequence","title":"Halton Sequence","text":"<p>One of the well-known low discrepancy sequences is generated using the radical inverse of numbers. They are called radical inverse sequence, and is defined as:</p> \\[ \\phi_b(i) = \\sum_{k\\ge 0}{d_{i,k}\\ b^{-1-k}} \\] <p>where \\(b\\) is the base and \\(d_k\\) is the \\(k\\)-th digit in the \\(b\\)-ary expansion of \\(n\\). To generate a sequence for \\(b=2\\), first represent the natural numbers in binary, then revert the digits and take its inverse. i.e.</p> \\[ \\begin{align} &amp;\\quad \\phi_2(1),\\phi_2(2),\\phi_2(3),\\phi_2(4),\\phi_2(5),\\phi_2(6),\\dots \\\\ &amp;=0.1_2, 0.01_2, 0.11_2, 0.001_2, 0.101_2, 0.011_2,\\dots \\\\ &amp;=\\frac12,\\frac14,\\frac34,\\frac18,\\frac58,\\frac38,\\dots \\\\ \\end{align} \\] <p>This is also known as the van der Corput sequence<sup>2</sup>, a specialized one-dimensional radical inverse sequence. The generalized form is called the Halton sequence, that is scalable to higher dimensions. To generate points in \\(N\\)-dimension, simply pick a different base from the prim number series \\({2, 3, 5, 7, 11, \\dots}\\). Scratchapixel has a more in-depth explanation, feel free to give it a read! </p> <p>Start Reset</p>"},{"location":"notes/sampling-techniques/#sobol-sequence","title":"Sobol Sequence","text":""},{"location":"notes/sampling-techniques/#progressive-multi-jittered-sample-sequence","title":"Progressive Multi-Jittered Sample Sequence<sup>3</sup>","text":"<ol> <li> <p>Dalal, I., Stefan, D., &amp; Harwayne-Gidansky, J. (2008). Low discrepancy sequences for Monte Carlo simulations on reconfigurable platforms. In 2008 International Conference on Application-Specific Systems, Architectures and Processors (pp. 108\u2013113).\u00a0\u21a9</p> </li> <li> <p>van der Corput, J.G. (1935), \"Verteilungsfunktionen (Erste Mitteilung)\" (PDF), Proceedings of the Koninklijke Akademie van Wetenschappen te Amsterdam (in German), 38: 813\u2013821, Zbl 0012.34705\u00a0\u21a9</p> </li> <li> <p>Christensen, P., Kensler, A., &amp; Kilpatrick, C. (2018). Progressive Multi-Jittered Sample Sequences. Computer Graphics Forum.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/mathematics/measure-theory/","title":"Measure Theory","text":""},{"location":"notes/mathematics/measure-theory/#preface","title":"Preface","text":"<p>I saw an interesting argument on the internet the other day, claiming that measure theory should be a part of the high school curriculum before touching any topics in statistics or calculus. That sparks my curiosity in learning what the measure theory is actually about. I thought it would be a good start to read through Terence Tao's book \"An Introduction to Measure Theory\". Hopefully this gives me better insights into what measurement is about in the mathematic world. </p>"},{"location":"notes/mathematics/measure-theory/#problem-of-measure","title":"Problem of Measure","text":"<p>What is a measure? The first thing comes to my mind was to use a ruler, scale, etc., something with a discrete interval to physically measure a body \\(E\\). It's either length of a rope, or weight of a rock. Doesn't matter. It's just about how to obtain a quantitative measurement of something, a magnitude.</p> <p>But in the real world, everything is made of immeasurable atomic components. Think of the Coastline Paradox. So to formalize this intuition would be defining a solid body as consisting an infinite and uncountable number of points, each of which has a measure of zero. Even though this sounds reasonable at first, it causes two problems:</p> <ol> <li>\\(0\\cdot\\infty\\) is undefined, we simply couldn't measure it mathematically.</li> <li>Two bodies that have exactly the same number of points, need not have the same measure. Imagine a rope of length 1, you disassemble it into an uncountable number of points and reassemble it twice further apart, now you got the same rope but with length of 2.</li> </ol> <p>If a measurement is infinitesimally small, what is its length? No one governs its length, right?</p> <p>Ones might blame the infinite partitions that causes all sorts of weirdness. But the Banach-Tarski paradox uses a combination of set theory to demonstrate that it's theoretically possible to disassemble a unit sphere \\(B := \\{(x,y,z) \\in \\mathbf{R}^3: x^2+y^2+z^2\\leq1\\}\\) into 5 pieces, which can then be reassembled by rigid motion (translation/rotation) to form two disjoint copies of the original sphere. </p> <p>My head is about to explode when I read until here. I'm sure everyone is. This is why mathematicians try not to think about them. These ideas are considered highly pathological and considered as not having much practical applications in mathematics. Therefore, the modern problem of measure has shifted the focus to a certain subset of the non-pathological cases in \\(\\mathbf{R}^d\\), which mathematicians refer as the measurable sets. Then the following subproblem arose:</p> <ol> <li>What does it mean for a subset \\(E\\) of \\(\\mathbf{R}^d\\) to be measurable?</li> <li>If a set \\(E\\) is measurable, how does one define its measure?</li> <li>What nice properties or axioms does measure obey?</li> <li>Are ordinary sets (e.g. cubes, sphere, platonic solids, etc.) measurable?</li> <li>Does the measure of an ordinary set equal the naive geometric measure?</li> </ol>"},{"location":"notes/mathematics/measure-theory/#lebesgue-integral","title":"Lebesgue Integral","text":"<p>Most continuous functions can be integrated using a Riemann integral, i.e. using infinitesimal rectangles to approximate the area under a curve. For example, a simple function like:</p> \\[ f(x) =  \\begin{cases} 0 \\quad x \\in [0, 1] \\end{cases} \\] <p>Of course its integral is zero, since every rectangles have a height of zero. But what if the function is one exclusively at both ends?</p> \\[ f(x) =  \\begin{cases} 1 \\quad x = 0\\\\ 0 \\quad x \\in (0, 1)\\\\ 1 \\quad x = 1 \\end{cases} \\] <p>Despite it's a discontinuous function, this still can be evaluated by a Riemann integral. Since the rectangle width at both ends tends to zero, their areas are still zero. And hence the whole integral evaluates to zero.</p> <p>Following the same logic, no matter how many points in the interval [0, 1] that I disconnect from the function and set its value to 1, the integral will always evaluates to be zero. Or, is it? </p> <p>Consider the following:</p> \\[ \\mathbf{1}_\\mathbb{Q}(x) =  \\begin{cases} 1 \\quad x \\in \\mathbb{Q}\\\\ 0 \\quad x \\notin \\mathbb{Q} \\end{cases} \\] <p>This is called a Dirichlet function. It is an example of a pathological function where it's 1 if \\(x\\) is a rational number and 0 if \\(x\\) is irrational. </p> <p>This cannot be solved by the Riemann integral due to rational and irrational number are closely packed on the real number line. No matter how small the interval around a rational number (width of a infinitesimal rectangle), you will always capture an infinite number of irrational numbers around the rational number.</p> <p>This is where the Lebesgue integral comes to rescue. It's base on the axiom that the measure of a single point on the real number line is zero. This is denoted by:</p> \\[ \\mu(\\text{point})=0 \\] <p>If we just look at all the rational numbers, since they are just individual points with 0 measurement, their sum is going to be zero (regardless of its defined value in the function). For the irrationals, their sum is zero too. So the entire integral is zero.</p> <p>However, when we swap the rationals and irrationals in the Dirichlet function. That is,</p> \\[ \\mathbf{1}_\\mathbb{Q}(x) =  \\begin{cases} 1 \\quad x \\notin \\mathbb{Q}\\\\ 0 \\quad x \\in \\mathbb{Q} \\end{cases} \\] <p>Noted that the rationals still evaluates to zero due to the zero measurement of a point as discussed above. What different is the irrationals. We all know the integral length of [0, 1] is one, and the fact that real numbers is a union of rational and irrational numbers (i.e. \\(\\mathbb{R}=\\text{rational}\\ \\cap\\ \\text{irrational}\\)). Thus, the integral must be 1 because the irrational set must be the complement that make up the rest of the interval.</p> <p>Bizarre, I know.</p>"},{"location":"notes/mathematics/optimal-transport/","title":"Optimal Transport","text":"<p>It's hard to imagine there's a distance between two distinct probability distributions. It is theoretically possible to flow from one distribution to the other through a transportation. Optimal transport in this context, simply means finding the shortest distance in such transportation that reaches the other distribution. These measurement of distances are often refer to divergence.</p>"},{"location":"notes/mathematics/optimal-transport/#divergence","title":"Divergence","text":""},{"location":"notes/mathematics/optimal-transport/#kullback-lieibler-divergence","title":"Kullback-Lieibler Divergence","text":"\\[ D_{KL}(P||Q) = \\int{ p(x)\\ log\\left(\\frac{p(x)}{q(x)}\\right) } dx \\] <p>You may also think of \\(P\\) and \\(Q\\) are probability measures on a measurable space where \\(x\\in\\mathcal{X}\\). Then the divergence is said to be the relative entropy from \\(Q\\) to \\(P\\).</p>"},{"location":"notes/mathematics/projective-geometric-algebra/","title":"Projective Geometric Algebra","text":""},{"location":"notes/mathematics/running-statistics/","title":"Running Statistics","text":"<p>People often use running statistics in finance to model a never-ending sequence of data. I use it mostly for stochastic Monte Carlo simulation. </p>"},{"location":"notes/mathematics/running-statistics/#welfords-algorithm","title":"Welford's Algorithm","text":"\\[ \\begin{align*} \\bar{x}_n &amp;= \\frac{1}{n}\\sum_{i=1}^n{x_i} &amp; &amp;= \\frac{(n-1)\\bar{x}_{n-1}+x_n}{n}\\\\ \\sigma_n^2 &amp;= \\frac{1}{n}\\sum_{i=1}^n{(x_i-\\bar{x}_n)^2} &amp; &amp;= \\frac{(n-1)\\sigma_{n-1}^2+(x_n-\\bar{x}_{n-1})(x_n-\\bar{x}_n)}{n}\\\\ \\end{align*} \\] <p>Precision proof version:</p> \\[ \\begin{align*} \\bar{x}_n &amp;= \\bar{x}_{n-1} + \\frac{x_n-\\bar{x}_{n-1}}{n}\\\\ \\sigma_n^2 &amp;= \\sigma_{n-1}^2 + \\frac{(x_n-\\bar{x}_{n-1})(x_n-\\bar{x}_n) - \\sigma_{n-1}^2}{n}\\\\ \\end{align*} \\]"},{"location":"notes/mathematics/running-statistics/#rust-example","title":"Rust Example","text":"<pre><code>use std::ops::{Add, Div, Mul, Sub};\n\n#[derive(Default)]\nstruct WelfordOnlineStats&lt;T&gt; {\n    pub mean: T,\n    pub variance: T,\n    pub count: usize,\n}\nimpl&lt;T&gt; WelfordOnlineStats&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt;\n        + Sub&lt;Output = T&gt;\n        + Mul&lt;Output = T&gt;\n        + Div&lt;f32, Output = T&gt;\n        + Copy\n        + Clone,\n{\n    fn add(&amp;mut self, value: T) {\n        if self.count == 0 {\n            self.mean = value;\n            self.count += 1;\n        } else {\n            self.count += 1;\n            let diff: T = value - self.mean;\n            self.mean = self.mean + diff / self.count as f32;\n            self.variance = self.variance\n                + (diff * (value - self.mean) - self.variance) / (self.count - 1) as f32;\n        }\n    }\n}\n</code></pre>"},{"location":"notes/mathematics/spherical-harmonics/","title":"Spherical Harmonics","text":"<p>If you know what Fourier series are, spherical harmonics are very similar periodic basis functions. But rather on the time domain, it's defined on the surface of a sphere \\(S^2\\). </p>"},{"location":"notes/mathematics/spherical-harmonics/#analogy-to-fourier-series","title":"Analogy to Fourier series","text":"<p>Let's start small. Fourier series is defined as the expansion of a periodic function into a sum of trigonometric functions<sup>1</sup>. So it's just a sum of basis wave functions (i.e. \\(\\sin\\) and \\(\\cos\\)). It's often expressed in the time domain, which is a single variable only. </p> <p>Here is the square wave fourier series expansion up to \\(n\\) terms. It's defined as:</p> \\[ y=\\frac{4}{\\pi} \\sum_{k=1}^n {\\frac{\\sin((2k-1)x)}{2k-1}} \\] <p>Now imagine transferring the square waves onto a unit circle \\(S^1\\). I guess you could call it \"Circular Harmonics\"<sup>2</sup>, if you really want to give it a name. </p> <p>But at the end, this is still a Fourier series, just with a different parameterization. Noted that \\(x\\) is now mapped to the angular part \\(\\theta \\in [0, 2\\pi)\\), and \\(y\\) is mapped to the radial part \\(r\\) in a polar coordinate system. There is no significant difference from our first equation.</p> \\[ r = \\frac{4}{\\pi} \\sum_{k=1}^n {\\frac{\\sin((2k-1)\\theta)}{2k-1}} \\] <p>I purposely color the positive and negative lobes in two different colors, because in one sine function cycle, \\([0, \\pi]\\) is positive and \\([\\pi, 2\\pi]\\) is negative. When it's wrapped around a circle, they become distinct lobes carrying opposite sign.</p> <p>Let's break the series back to its basis functions. That is, without the scaling, we now truly obtained the \"Circular Harmonics\" bases. And here is how they look.</p> \\[ r=\\sin(k\\theta) \\] <p>Looks a lot like spherical harmonics, right? With these basis functions, they can theoretically form any functions if it's an infinite series expansion. But realistically, in most applications, only a few of them are needed to approximate/fit a smooth circular function. </p> <p>Circular Harmonics Rotation</p> <p>One interesting observation about rotating these basis functions, is that the choice of the oscillating/trigonometric function. Recall that \\(\\sin(\\theta)=\\cos(\\frac{\\pi}{2}-\\theta)\\), meaning cosine is just an off-phase sine. Therefore, substituding the sine to cosine is all we need to rotate it by \\(90^{\\circ}\\).</p>"},{"location":"notes/mathematics/spherical-harmonics/#derivation-of-spherical-harmonics","title":"Derivation of Spherical Harmonics","text":"<p>Spherical harmonics originate from solving Laplace's equation in the spherical domains. Functions that are solutions to Laplace's equation are called harmonics<sup>3</sup>. </p> <p>To recap Laplace's equation is to solve for a function \\(f\\) where its divergence of gradient is 0, e.g. \\(\\nabla^2f=0\\). In Cartesian coordinates, the three-dimensional Laplacian is defined as:</p> \\[ \\nabla^2 f = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2} + \\frac{\\partial^2f}{\\partial z^2} \\] <p>In spherical coordinates \\(x=r\\sin\\theta\\cos\\phi,\\ y=r\\sin\\theta\\sin\\phi,\\ z=r\\cos\\theta\\), it became: (full derivation)</p> \\[ \\begin{align*} \\nabla^2f &amp;= \\frac{1}{r^2\\sin\\theta}   \\left(     \\frac{\\partial}{\\partial r} r^2 \\sin\\theta \\frac{\\partial}{\\partial r} + \\frac{\\partial}{\\partial \\theta} \\sin\\theta \\frac{\\partial}{\\partial\\theta} + \\frac{\\partial}{\\partial\\phi} \\csc\\theta \\frac{\\partial}{\\partial\\phi}   \\right) \\\\   &amp;= \\frac{1}{r^2}   \\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial f}{\\partial r}\\right) +    \\frac{1}{r^2\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial f}{\\partial\\theta}\\right) +    \\frac{1}{r^2\\sin^2\\theta} \\frac{\\partial^2 f}{\\partial\\phi^2} \\end{align*} \\] <p>To separate the radial part (\\(r\\)) from the angular part (\\(\\theta, \\phi\\)), we need to let \\(f(r, \\theta, \\phi)=R(r)Y(\\theta, \\phi)\\) and perform what's called a separation of variables.</p> \\[ \\begin{align*}     &amp;\\ \\nabla^2 (R(r)Y(\\theta, \\phi))\\\\    = &amp;\\ \\frac{1}{r^2} \\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)Y(\\theta, \\phi)}{\\partial r}\\right) +    \\frac{1}{r^2\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial R(r)Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{r^2\\sin^2\\theta} \\frac{\\partial^2 R(r)Y(\\theta, \\phi)}{\\partial\\phi^2}\\\\   = &amp;\\cancel{\\frac{1}{r^2}}\\ Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) +    \\cancel{\\frac{1}{r^2}}R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)\\\\   = &amp;\\ Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) + R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) \\end{align*} \\] <p>Multiply both side by \\(\\frac{1}{R(r)Y(\\theta, \\phi)}\\):</p> \\[ \\begin{align*}   Y(\\theta, \\phi)\\left(\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)\\right) + R(r)\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) &amp;= 0   \\\\   \\frac{\\cancel{Y(\\theta, \\phi)}}{R(r)\\cancel{Y(\\theta, \\phi)}}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right) + \\frac{\\cancel{R(r)}}{\\cancel{R(r)}Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right) &amp;= \\frac{1}{R(r)Y(\\theta, \\phi)} \\cdot 0   \\\\   \\underbrace{\\frac{1}{R(r)}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)}_{r\\text{-dependent}} + \\underbrace{\\frac{1}{Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)}_{\\theta\\phi\\text{-dependent}} &amp;= 0 \\end{align*} \\] <p>Now we can introduce a separation constant \\(\\ell(\\ell+1)\\) such that both \\(r\\text{-dependent}\\) and \\(\\theta\\phi\\text{-dependent}\\) parts still adds up to 0, satisfying the original Laplace's equation.</p> \\[ \\begin{cases}   \\frac{1}{R(r)}\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right)   = \\ell(\\ell+1)\\\\   \\frac{1}{Y(\\theta, \\phi)}\\left(   \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) +    \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}\\right)   =   -\\ell(\\ell+1) \\end{cases} \\] <p>Then we have successfully obtained two separated ordinary differential equations, the radial part \\(\\eqref{1}\\) and the angular part \\(\\eqref{2}\\).</p> \\[ \\begin{align*}   &amp;\\frac{\\partial}{\\partial r} \\left(r^2 \\frac{\\partial R(r)}{\\partial r}\\right) = \\ell(\\ell+1)R(r) &amp;\\tag{1}\\label{1}\\\\   &amp;\\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2} = -\\ell(\\ell+1)Y(\\theta, \\phi) &amp;\\tag{2}\\label{2} \\end{align*} \\] <p>Spherical harmonics arises from the angular part \\(Y(\\theta, \\phi)\\) of a spherical Laplace's equation. Those \"harmonics\" are forms an infinite set of solutions that satisfy the angular part of the Laplace's equation. Therefore, the radial part \\(R(r)\\) can be omitted for now.</p> Linearity of Laplace's equation and relation to SH <p>The Laplace operator \\(\\nabla^2\\) is a linear operator since it satisfies the superposition principle, where the sum of any two solutions is also a solution. That is, \\(\\nabla^2(a\\theta + b\\phi) = a\\nabla^2(\\theta) + b\\nabla^2(\\phi)\\). The only prerequisite is that \\(\\nabla^2(\\theta)=0\\) and \\(\\nabla^2(\\phi)=0\\), and thus proofing its linearity (i.e. \\(a\\cdot0+b\\cdot0=0\\)).</p> \\[ \\underbrace{\\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial Y(\\theta, \\phi)}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 Y(\\theta, \\phi)}{\\partial\\phi^2}}_{\\nabla^2 f} = \\underbrace{-\\ell(\\ell+1)}_{\\text{eigenvalue}(\\lambda)}\\underbrace{Y(\\theta, \\phi)}_{\\text{eigenfunction}(f)} \\] <p>Taking equation \\((2)\\) and simplify it to \\(\\nabla^2(Y(\\theta, \\phi)) = -\\ell(\\ell+1)Y(\\theta, \\phi)\\). It's not hard to see that this takes the form of \\(Df=\\lambda f\\), where \\(D\\) is a linear operator<sup>4</sup><sup>5</sup>. And just so happened the Laplace operator \\(\\nabla^2\\) is also a linear operator! So, our separation constant \\(-\\ell(\\ell+1)\\) are actually eigenvalues and the spherical harmonics \\(Y(\\theta, \\phi)\\) are indeed eigenfunctions.</p> <p>By writing \\(Y(\\theta, \\phi)=\\Theta(\\theta)\\Phi(\\phi)\\), we are going to perform the separation of variable again. That gives us:</p> \\[ \\begin{align*} \\frac{1}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial (\\Theta(\\theta)\\Phi(\\phi))}{\\partial\\theta}\\right) + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2 (\\Theta(\\theta)\\Phi(\\phi))}{\\partial\\phi^2} &amp;= -\\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi))\\\\ \\frac{\\Phi(\\phi)}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\frac{\\Theta(\\theta)}{\\sin^2\\theta} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2} + \\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi)) &amp;= 0\\\\ \\end{align*} \\] <p>Multiplying both sides by \\(\\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\) <sup>6</sup>:</p> \\[ \\begin{align*} \\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\left(\\frac{\\Phi(\\phi)}{\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\frac{\\Theta(\\theta)}{\\sin^2\\theta} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2} + \\ell(\\ell+1)(\\Theta(\\theta)\\Phi(\\phi))\\right) &amp;= \\frac{\\sin^2\\theta}{\\Theta(\\theta)\\Phi(\\phi)}\\cdot0\\\\ \\underbrace{\\frac{\\sin\\theta}{\\Theta(\\theta)} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\ell(\\ell+1)\\sin^2\\theta}_{\\theta\\text{-dependent}} + \\underbrace{\\frac{1}{\\Phi(\\phi)} \\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2}}_{\\phi\\text{-dependent}} &amp;= 0 \\tag{3}\\label{3}\\\\ \\end{align*} \\] <p>This time let \\(m^2\\) be the separation constant:</p> \\[ \\begin{align*}   &amp;\\frac{\\sin\\theta}{\\Theta(\\theta)} \\frac{\\partial}{\\partial \\theta} \\left(\\sin\\theta \\frac{\\partial \\Theta(\\theta)}{\\partial\\theta}\\right) + \\ell(\\ell+1)\\sin^2\\theta = m^2 &amp;\\tag{4}\\label{4} \\\\   &amp;\\frac{1}{\\Phi(\\phi)}{\\frac{\\partial^2 \\Phi(\\phi)}{\\partial\\phi^2}} = -m^2 &amp;\\tag{5}\\label{5} \\end{align*} \\] <p>Fortunately, \\(\\eqref{5}\\) already has solutions of \\(\\Phi(\\phi)=e^{\\pm im\\phi}\\), which is the two-dimensional angular Laplacian. Substituding it back into \\(\\eqref{3}\\) and solve for the \\(\\theta\\text{-dependent}\\) portion, we get the solutions of \\(\\Theta(\\theta)=P_\\ell^m(\\cos\\theta)\\) in the form of a Legendre polynomial<sup>6</sup><sup>7</sup> \\(P_\\ell^m(x)\\). (1)</p> <ol> <li>I have zero clue on this part about Legendre polynomials. Come back here when I am prepared.</li> </ol> \\[ P_\\ell^m(x) = \\frac{(-1)^m}{2^\\ell \\ell!}(1-x^2)^\\frac{m}{2} \\frac{\\partial^{\\ell+m}}{\\partial x^{\\ell+m}} (x^2-1)^\\ell \\] <p>Noted that this formula must have \\(\\ell \\geq 0\\) and \\(m\\) being integers such that \\(|m| \\leq \\ell\\). </p> <p>With that, we get the solution to \\(\\eqref{2}\\), which is the spherical harmonics \\(Y(\\theta, \\phi)=P_\\ell^m(\\cos\\theta)e^{\\pm im\\phi}\\). And just like vectors, to enforce orthonormality, a normalization factor is required to make independent spherical harmonics orthonormal.</p> \\[ Y_\\ell^m(\\theta, \\phi) = \\underbrace{\\sqrt{\\frac{2\\ell+1}{4\\pi}\\frac{(\\ell-m)!}{(\\ell+m)!}}}_{\\text{normalization factor}} P_\\ell^m(\\cos\\theta)e^{\\pm im\\phi} \\] <p>\\(Y(\\theta, \\phi)\\) denotes the whole sets of infinitely many solutions to the angular Laplacian. With this notation \\(Y_\\ell^m(\\theta, \\phi)\\) means it's a specific spherical harmonic with order \\(\\ell\\) and degree \\(m\\). </p> <ol> <li> <p>Fourier series (2024). In Wikipedia. https://en.wikipedia.org/wiki/Fourier_series \u21a9</p> </li> <li> <p>Circular Harmonics: Digging in circles (2021). Jon Vald\u00e9s. https://valdes.cc/articles/ch.html \u21a9</p> </li> <li> <p>Spherical harmonics (2024). In Wikipedia. https://en.wikipedia.org/wiki/Spherical_harmonics \u21a9</p> </li> <li> <p>Eigenfunction (2024). In Wikipedia. https://en.wikipedia.org/wiki/Eigenfunction \u21a9</p> </li> <li> <p>All You Need to Know about Spherical Harmonics. Mathcube. https://www.cantorsparadise.com/all-you-need-to-know-about-spherical-harmonics-29ff76e74ad5 \u21a9</p> </li> <li> <p>Spherical Harmonic. Weisstein, Eric W. From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/SphericalHarmonic.html \u21a9\u21a9</p> </li> <li> <p>Associated Legendre Polynormial. Weisstein, Eric W. From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/AssociatedLegendrePolynomial.html \u21a9</p> </li> </ol>"},{"location":"notes/simulation/force-based/","title":"Force Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/impulse-based/","title":"Impulse Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/numerical-methods/","title":"Numerical Methods","text":"<p>In this chapter, I will cover different methods to integrate differential equations (ODE) numerically. Since the integrand is evaluated numerically, I will replace every \\(=\\) to be \\(\\approx\\) to make it clear it's just an estimation only.</p> <p>// TODO: investigate what is consistency and order in numerical methods</p>"},{"location":"notes/simulation/numerical-methods/#euler-method","title":"Euler Method","text":"<p>This is a first-order numerical method to solve ordinary differential equations (ODEs) with a given initial value. It allows us to approximate a nearby point on the curve by moving a short distance \\(\\Delta t\\) along a line tangent, that is the first-order derivative \\(f'(t)\\), to the curve \\(f(t)\\). To write that down:</p> \\[ \\begin{cases} f'(t) &amp;\\approx \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t) \\Delta t \\end{cases} \\] <p>Rewriting it in an iterative way, we can find the next position a particle \\(\\mathbf{x}_{n+1}\\) with its current position \\(\\mathbf{x}_n\\):</p> \\[ \\mathbf{x}_{n+1} \\approx \\mathbf{x}_n + \\dot{\\mathbf{x}}(t) \\Delta t \\]"},{"location":"notes/simulation/numerical-methods/#implicit-euler-method","title":"Implicit Euler Method","text":"<p>The implicit here means instead of computing directly the approximation \\(\\mathbf{x}_{n+1}\\) from \\(\\mathbf{x}_n\\), we need to solve an implicit equation:</p> \\[ \\begin{cases} f'(t) &amp;\\approx \\frac{f(t) - f(t-\\Delta t)}{\\Delta t}\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t+\\Delta t)\\Delta t \\end{cases} \\] <p>Also known as backward Euler method, because it uses right-hand quadrature method instead of the left-hand rectangle. Implicit method is difficult to evaluate, and usually required iterative methods to solve (e.g. fixed point iteration), but they have better numerical properties and thus more stable.</p>"},{"location":"notes/simulation/numerical-methods/#symplectic-euler-method","title":"Symplectic Euler Method","text":"<p>This one has way too much names, it's also called semi-implicit Euler, semi-explicit Euler, Euler-Cromer, and Newton-St\u00f8rmer-Verlet<sup>1</sup>. </p> \\[ \\begin{cases} f'(t+\\Delta t) &amp;\\approx f'(t) + f''(t)\\Delta t\\\\ f(t+\\Delta t) &amp;\\approx f(t) + f'(t+\\Delta t)\\Delta t \\end{cases} \\] <p>Rewrite it in terms of motion:</p> \\[ \\begin{cases} \\mathbf{v}_{n+1} &amp;\\approx\\mathbf{v}_n + \\mathbf{a}_{n}\\Delta t\\\\ \\mathbf{x}_{n+1} &amp;\\approx\\mathbf{x}_n + \\mathbf{v}_{n+1}\\Delta t\\\\ \\end{cases} \\] \\[ \\begin{align} \\therefore \\mathbf{x}_{n+1}&amp;\\approx\\mathbf{x}_n + (\\mathbf{v}_n + \\mathbf{a}_{n}\\Delta t)\\Delta t\\\\ &amp;\\approx\\mathbf{x}_n + \\mathbf{v}_n\\Delta t + \\mathbf{a}_{n}\\Delta t^2 \\end{align} \\] <p>Look familiar? This is usually seen in game physics to predict a moving body in the next time frame. It is a mix of explicit and implicit method to calculate the final body position. Velocity is integrated explicitly, and position is integrated implicitly by using the velocity from the next time frame. </p>"},{"location":"notes/simulation/numerical-methods/#verlet-method","title":"Verlet Method<sup>2</sup>","text":"<p>This is a second-order numerical method to integrate Newton's equations of motion. </p> <p>We begin by letting the second-derivative \\(\\ddot{\\mathbf{x}}=f''(t)\\) be the change of the finite difference of the backward-difference and forward-difference. In other words, using central difference approximation to find the second derivative. When \\(\\Delta t\\rightarrow 0\\), it will converge to the true second-derivative value. In numerical computations, it will always be an approximation as it's impossible to have an infinitely small step size. </p> <p>Animate</p> \\[ \\begin{align} f''(t) &amp;\\approx \\frac{\\frac{f(t+\\Delta t)-f(t)}{\\Delta t}-\\frac{f(t)-f(t-\\Delta t)}{\\Delta t}}{\\Delta t}\\\\ &amp;\\approx \\frac{\\frac{(f(t+\\Delta t)-f(t))-(f(t)-f(t-\\Delta t))}{\\Delta t}}{\\Delta t}\\\\ &amp;\\approx \\frac{f(t+\\Delta t)-2f(t)+f(t-\\Delta t)}{\\Delta t^2} \\tag{1} \\end{align} \\] <p>It has an interesting property, we can see the approximated second-derivative is independent from its first-derivative. In simpler words, the next position vector \\(\\mathbf{x}_{n+1}\\) can be obtained directly by the previous two \\(\\mathbf{x}_n\\) and \\(\\mathbf{x}_{n-1}\\), without the need of calculating velocity. Continue from \\((1)\\), we get:</p> \\[ \\begin{align} \\ddot{\\mathbf{x}} &amp;\\approx \\frac{\\mathbf{x}_{n+1} - 2\\mathbf{x}_n + \\mathbf{x}_{n-1}}{\\Delta t^2}\\\\ \\ddot{\\mathbf{x}} {\\Delta t^2} &amp;\\approx \\mathbf{x}_{n+1} - 2\\mathbf{x}_n + \\mathbf{x}_{n-1}\\\\ \\mathbf{x}_{n+1}&amp;\\approx2\\mathbf{x}_n - \\mathbf{x}_{n-1} + \\ddot{\\mathbf{x}} \\Delta t^2 \\tag{2} \\end{align} \\]"},{"location":"notes/simulation/numerical-methods/#computing-velocity","title":"Computing Velocity","text":"<p>Because velocities are not explicitly given by definition, it can only be estimated by the mean value theorem.</p> \\[ f'(t) \\approx \\frac{f(t+\\Delta t) - f(t-\\Delta t)}{2\\Delta t} \\] <p>To erase the dependency or previous position, just shift the time frame by \\(\\Delta t\\) at the cost of accuracy (forward-difference approximation). And you will soon notice it falls back to the form of Euler method.</p> \\[ \\begin{align} f'(t+\\Delta t) &amp;\\approx \\frac{f(t+2\\Delta t) - f(t)}{2\\Delta t}\\\\ &amp;\\approx \\frac{f(t+\\Delta t) - f(t)}{\\Delta t}\\\\ \\end{align} \\] <ol> <li> <p>F.Crivelli. The St\u00f6rmer-Verlet method, 2008. https://www2.math.ethz.ch/education/bachelor/seminars/fs2008/nas/crivelli.pdf \u21a9</p> </li> <li> <p>Gorilla Sun. Euler and Verlet Integration for Particle Physics. https://www.gorillasun.de/blog/euler-and-verlet-integration-for-particle-physics/ \u21a9</p> </li> </ol>"},{"location":"notes/simulation/position-based-dynamics/","title":"Position Based Dynamics","text":"<p>As oppose to any other rigidbody simulations, force-based and impulse-based that deals with velocity and acceleration calculations, position-based simulation<sup>1</sup> attempts to minimize the constrains on the position domain down to particle level. </p>"},{"location":"notes/simulation/position-based-dynamics/#problem","title":"Problem","text":"<p>Given a set of \\(M\\) constrains (basically means there are \\(M\\) equality or inequality equations to satisfy), we need to solve for \\(N\\times\\mathbb{R}^3\\) unknowns to resolve our final positions. Most of the time, the number of constraints won't match the number of unknowns we are solving (i.e. \\(M \\neq N\\)). This means if the problem was a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), the solution can't be obtained easily by inverting the matrix and solve for \\(\\mathbf{x}\\). Not to mention the system we are solving won't necessarily be linear. For example, a simple distance constraint \\(C_i(\\mathbf{x}_1, \\mathbf{x}_2)=\\left|\\mathbf{x}_1-\\mathbf{x}_2\\right|^2-d^2\\) alone is a non-linear equation.</p> <p>Thus, it all boils down to a problem of finding a set of positions \\(\\mathbf{x}\\) that minimize if not solved the constrained system:</p> \\[ C(\\mathbf{x}) = \\begin{cases} C_1(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_{\\mathbf{n}_j}) \\succ 0\\\\ \\cdots\\\\ C_M(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_{\\mathbf{n}_j}) \\succ 0\\\\ \\end{cases} \\] <p>Where \\(\\mathbf{x}\\) is the concatenation of \\(N\\times\\mathbb{R}^3\\) positions we are trying to solve, and the symbol \\(\\succ\\) denotes either \\(=\\) or \\(\\geq\\). \\(C(\\mathbf{x})=0\\) means the constraint has a bilateral condition enforced, usually representing strong forces which should always hold true. On the other hand, \\(C(\\mathbf{x}) \\geq 0\\) or \\(C(\\mathbf{x}) \\leq 0\\) means the constraint is loosely enforced. They are usually seen in collision constraints where particles are free to move on one side, but preventing them to enter the other side. </p>"},{"location":"notes/simulation/position-based-dynamics/#gauss-seidel-iterative-method","title":"Gauss-Seidel Iterative Method","text":"<p>As said in the previous section, there won't be a closed-form solution because the system is neither symmetric nor linear. Our best bet is to apply an iterative solver to minimize the system after a fixed amount of iterations and hope for the best that the approximated result will satisfy all our constraints. This is where the non-linear Gauss-Seidel algorithm comes in.</p> <p>This is a kind of local optimization algorithm where  each constraints are solved separately. At the end of each iteration, each particles should have a correction vector \\(\\Delta\\mathbf{x}\\) in order to reach their constrained condition. Thus, after each step, the constraint value \\(C(\\mathbf{x}+\\Delta\\mathbf{x})\\) should be closer and closer to zero.</p> \\[ C(\\mathbf{x}+\\Delta\\mathbf{x})\\approx C(\\mathbf{x})+\\nabla C(\\mathbf{x})\\cdot\\Delta \\mathbf{x} \\rightarrow 0 \\tag{1}\\label{1} \\] <p>The above equation can be thought as a Newton iteration, where the constraint function is linearized and we are stepping the gradient \\(\\nabla C(\\mathbf{x})\\) with a step size \\(\\Delta\\mathbf{x}\\). Hence, the position correction vector \\(\\Delta\\mathbf{x}\\) will then be expressed as the gradient scaled by the Lagrange multiplier as follows:</p> \\[ \\Delta\\mathbf{x}=\\lambda \\nabla C(\\mathbf{x}) \\] <p>The scalar \\(\\lambda\\) is found by substituting \\(\\Delta\\mathbf{x}\\) back into the equation \\(\\eqref{1}\\).</p> \\[ \\begin{align*} C(\\mathbf{x})+\\nabla C(\\mathbf{x})\\cdot\\Delta\\mathbf{x} &amp;= 0\\\\ \\nabla C(\\mathbf{x})\\cdot\\Delta\\mathbf{x} &amp;= -C(\\mathbf{x})\\\\ \\Delta\\mathbf{x} &amp;= -\\frac{C(\\mathbf{x})}{\\|\\nabla C(\\mathbf{x})\\|^2} \\nabla C(\\mathbf{x}) \\tag{2}\\label{2} \\end{align*} \\] <p>You can clearly see, our Langrage multiplier \\(\\lambda\\) is the simply the coefficient part \\(-\\frac{C(\\mathbf{x})}{\\|\\nabla C(\\mathbf{x})\\|^2}\\). As each constraint is formed by a set of vertices \\(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{j}\\), we will have to compute \\(C(\\mathbf{x})\\) and \\(\\nabla C(\\mathbf{x})\\) with respect to each particle weighted by their masses. Noted how weighted sum is used so basically weights can be unbounded.</p> \\[ \\begin{cases} \\lambda &amp;= -\\frac{C(\\mathbf{x})}{\\sum{w_j\\|\\nabla_{\\mathbf{x}_j}C(\\mathbf{x})\\|^2}}\\\\ \\Delta\\mathbf{x}_j &amp;= \\lambda \\nabla_{\\mathbf{x}_j} C(\\mathbf{x}) w_j \\end{cases} \\]"},{"location":"notes/simulation/position-based-dynamics/#conservation-of-linear-momentum","title":"Conservation of Linear Momentum","text":"<p>Thonk</p> <p>The paper mentioned that by restricting the step direction to be \\(\\nabla C(\\mathbf{x})\\), satisfies the requirement for linear and angular momentum conservation. It also means only Lagrange multiplier scalar \\(\\lambda\\) has to be found to solve the correction equation \\(\\eqref{1}\\).I can't comprehend this part</p>"},{"location":"notes/simulation/position-based-dynamics/#algorithm","title":"Algorithm","text":"<p>The main position based dynamics (PBD) algorithm can be split into three different stages: prediction, solving constraints, and post-solve updates.</p> \\[ \\begin{align} &amp;\\mathbf{while}\\ \\text{simulating}\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow \\mathbf{v}_i + \\mathbf{f}_{ext}\\Delta t\\\\ &amp;\\quad\\quad \\mathbf{p}_i \\leftarrow \\mathbf{x}_i + \\mathbf{v}_i\\Delta t\\\\ &amp;\\quad \\mathbf{for} \\text{ all constraints } C\\\\ &amp;\\quad\\quad solve(C, \\Delta t)\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow (\\mathbf{x}_i - \\mathbf{p}_i) \\frac{1}{\\Delta t}\\\\ &amp;\\quad\\quad \\mathbf{x}_i \\leftarrow \\mathbf{p}_i \\end{align} \\] <p>Noted that this non-linear constrained system is \"solved\" by iterating each constraints and optimizing them sequentially, which to me, doesn't really \"solve\" the system but rather find a close-enough solution. This is also why it's plausible that a particle will end up violating one of the constaints (possibly clipping through geometries) after the solver reached its maximum iterations. </p>"},{"location":"notes/simulation/position-based-dynamics/#result","title":"Result","text":"<ol> <li> <p>Jan Bender, Mattias M\u00fcller, Miles Macklin. Position-Based Simulation Methods in Computer Graphics, Eurographics 2015. http://mmacklin.com/EG2015PBD.pdf \u21a9</p> </li> </ol>"},{"location":"notes/simulation/position-based-fluids/","title":"Position Based Fluids","text":"<p>As discussed in Position Based Simulation, position-based dynamics provided the foundations of simulating lagrangian fluids. Remember PBD is all about how to put constraints on particles and how to resolve them. Similarly, we can model how fluid particles interact by posing some constraints on them.</p>"},{"location":"notes/simulation/position-based-fluids/#density-constraint","title":"Density Constraint","text":"<p>The density constraint<sup>1</sup> aims to model the compressibility on fluids by constraining the density (measuring the amount of mass per unit volume) of a single fluid particle.</p> \\[ C_i(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = \\frac{\\rho_i}{\\rho_o}-1 \\] \\[ \\rho_i = \\sum_j{m_j W(\\mathbf{x}_i - \\mathbf{x}_j, h)} \\] <p>Since we only care about local density around a given particle, the density constraint takes all its neighboring particle positions \\(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}\\), and sum up their masses to \\(\\rho_i\\). Noted that each particles are weighted by a smoothing function \\(W\\) that falls off radially from the center of the kernel, which we will discuss more in the smoothing kernel section. The ratio to its rest density \\(\\rho_o\\) is then evaluated, indicating whether the fluid is being compressed locally (e.g. \\(\\rho_i &gt; \\rho_o\\)). In a perfect equilibrium, \\(\\rho_i\\) should equal to \\(\\rho_o\\) and the constraint \\(C_i=0\\) will be satisfied, meaning this fluid particle is happy :)</p> \\[ C_{density}(\\mathbf{x}) = \\begin{cases} C_1(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = 0\\\\ \\cdots\\\\ C_N(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = 0\\\\ \\end{cases} \\] <p>Scaling up the above to a \\(N\\)-particle simulation, every particles will be assigned a density constraint, to ensure its local density remain the same at all time. This is why the bilateral condition \\(C(\\mathbf{x})=0\\) is used here. Once we run the solver for a decent number of iterations, every fluid particles should be happily resting at their stable positions.</p> <p>What if the density is lower than the rest condition (e.g. \\(\\rho_i &lt; \\rho_o\\))?</p> <p>There will be cases where a lone particle wandered into the void, where a few or even no neighboring particles are present in its surroundings. This posed a tensile instability in our simulation, which can cause clustering or clumping due to negative pressures.</p> <p>This can result in extreme correction vectors and can break the simulation. One solution is to clamp the density constraint result to zero to prevent negative pressure.</p> \\[ C_i(\\mathbf{x}_1, \\cdots, \\mathbf{x}_\\mathbf{n_j}) = \\max(0, \\frac{\\rho_i}{\\rho_o}-1) \\] <p>However, this also means the cohesion nature of fluid particles is gone, particles almost always repel with each other. So by clamping the constraint, we are also limiting ourselves from having this cohesive behavior.</p> <p>Luckily, in the paper \"SPH without a tensile instability\"<sup>2</sup>, an artificial pressure correction factor is proposed. Where \\(\\Delta\\vec{q}\\) is vector to a point at some fixed distance inside the smoothing kernel radius \\(h\\) and \\(k\\) is a small positive constant. \\(\\|\\Delta\\vec{q}\\|=\\{0.1h, \\cdots, 0.3h\\}\\), \\(k = 0.001\\) and \\(n = 4\\) work well in my case.</p> \\[ \\lambda_{corr}=-k\\cdot\\left(\\frac{W(\\vec{r}, h)}{W(\\Delta\\vec{q}, h)}\\right)^n \\]"},{"location":"notes/simulation/position-based-fluids/#smoothing-kernels","title":"Smoothing Kernels","text":"<p>Smoothing kernels are essential for integrating local density of a particle in an SPH simulation. A good kernel needs to fulfill a few requirements:</p> <ul> <li>Radially symmetric (i.e. only depends on distance)</li> <li>Finite support (i.e. particles should only affect up to a certain distance)</li> <li>Differentiable and integrates to 1</li> </ul> Kernel Type Function \\(W(\\vec{r}, h)\\) Smooth (2D) \\(W_{smooth}(\\vec{r}, h) = \\frac{35}{32\\pi h^7} \\begin{cases} (h^2-\\|r\\|^2)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Smooth (3D) \\(W_{smooth}(\\vec{r}, h) = \\frac{315}{64 \\pi h^9} \\begin{cases} (h^2-\\|r\\|^2)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (2D) \\(W_{spiky}(\\vec{r}, h) = \\frac{2}{\\pi h^4} \\begin{cases} (h-\\|r\\|)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (3D) \\(W_{spiky}(\\vec{r}, h) = \\frac{15}{\\pi h^6} \\begin{cases} (h-\\|r\\|)^3 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) <p>In order to attract or repel neighboring particles, we need to compute the gradient of the smoothing kernel \\(\\nabla W(\\vec{r}, h)\\).</p> Kernel Type Gradient \\(\\nabla W(\\vec{r}, h)\\) Smooth (2D) \\(\\nabla W_{smooth}(\\vec{r}, h) = -\\vec{r}\\frac{105}{16\\pi h^7} \\begin{cases} (h^2-\\|r\\|^2)^2 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Smooth (3D) \\(\\nabla W_{smooth}(\\vec{r}, h) = -\\vec{r}\\frac{945}{32 \\pi h^9} \\begin{cases} (h^2-\\|r\\|^2)^2 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (2D) \\(\\nabla W_{spiky}(\\vec{r}, h) = -\\frac{\\vec{r}}{\\|\\vec{r}\\|}\\frac{6}{\\pi h^4} \\begin{cases} (h-\\|r\\|)^2 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\) Spiky (3D) \\(\\nabla W_{spiky}(\\vec{r}, h) = -\\frac{\\vec{r}}{\\|\\vec{r}\\|}\\frac{45}{\\pi h^5} \\begin{cases} (h-\\|r\\|)^2 &amp; \\text{if } 0\\leq \\|r\\|\\leq h\\\\ 0 &amp; \\text{else} \\end{cases}\\)"},{"location":"notes/simulation/position-based-fluids/#bloopers","title":"Bloopers","text":"1 2 3 <ol> <li> <p>Macklin, M., &amp; M\u00fcller, M. (2013). Position based fluids. ACM Transactions on Graphics (TOG), 32(4), 1-12.\u00a0\u21a9</p> </li> <li> <p>Monaghan, J. J. (2000). SPH without a tensile instability. Journal of computational physics, 159(2), 290-311.\u00a0\u21a9</p> </li> </ol>"},{"location":"random/image-auto-mesh/","title":"Image Auto-Mesh","text":"<p>This is a part of the bigger project that I'm working on (Spoiler: ARAP). My goal is to automatically generate a triangulated mesh around an RGBA image. But I can already tell there are few obvious obstacles/problems along the way need to be tackled.</p> <ol> <li>Generating contours around the image is hard, especially if the edges aren't smooth, of which can occur naturally for irregular brush strokes.</li> <li>Contours cannot be self-intersecting, no one wants that.</li> <li>Triangle sizes and shape are really important. They must be mostly equiangular and evenly distributed to minimize texture stretching when deformed.</li> <li>... I will add more to this when I can think of anything</li> </ol>"},{"location":"random/image-auto-mesh/#contouring","title":"Contouring","text":"<p>Contour refers to both the exterior and interior polygonal chain that sits on top of a similar isovalue and form a level set. </p> <p>In the original ARAP paper, they uses marching squares to extract the contour. Of course it makes sense to treat the image alpha mask as a scalar field. And run the marching square algorithm over the pixel grid to extract the desire contour.</p> <p>That's exactly what I did. Quantizing the image's alpha channel with a threshold, iterate through each 2x2 pixel grid cell and apply the corresponding cell configuration. Yet, the result isn't too appealing...</p> <p>&lt;insert image later&gt;</p> <p>The extracted contour is very jagged like a coastline. Normally this wouldn't be a problem, because it's still a valid contour. But ideally, I'd like to deform a mesh with less vertices/triangles as less complex mesh = faster deformation. </p> <p>Honestly, we can apply some line segment decimation algorithms like the Ramer-Douglas-Perucker and Visvalingam-Whyatt algorithm. Yeah, they will do the job, but that's extra work. Not only it takes time to compute, but also it can open a can of worms with exceptional cases and potentially self-intersecting contour. </p> <p></p>"},{"location":"random/image-auto-mesh/#first-attempt","title":"First Attempt","text":"<p>So what I have in mind is some simpler, a naive approach. Just hear me out. </p> <p>What if... what if, like a shrinking plastic wrap, start with an N-gon, then find the closest intersection with the target shape. That has to work, right? </p> <p>The process goes like this: </p> <p>I get the center of the bounding box of all the opaque pixels as the center of the N-gon. Or, I can use the centroid of those pixels, not sure which is better. Then I construct a line segment running from the N-gon's vertex to the center as my \"ray\". After that, find the closest intersection with Differential Diagnosis (DDx) algorithm and step towards the center until it returns the distance to the closest opaque hit. Finally apply vertex displacement along the ray direction with the given hit distance.</p> <p>But of course we don't want the final constructed mesh to \"eat\" away parts of the image, so padding was necessary. Adding padding is simple, just subtract a certain amount from the closest hit distance. And the end result looks like this.</p> <p></p> <p>And when you really think about it, it will NEVER generate a self-intersecting contour. Think of a pizza, shrinking the N-gon vertices is just like biting away an uncut pizza's crust (by the way, that's not how you eat a pizza). My point is, it will always remain a non-intersecting polygon. </p> <p>The only criticisms are, </p> <ol> <li>Line segments are non-uniform. Concave areas have steep edges because of the large differences in DDx hit distance. </li> <li>Elongated/spiky shapes can have extruded areas where the insufficiently subdivided N-gon can totally missed the intersection.</li> </ol> <p>For the non-uniform edges, I could either run an iterative segment relaxation process until they are evenly distributed, or better approach is to subdivide and interpolate the curve if it exceeded the maximum edge length. </p> <p>Except for the second point, yeah, I'd admit this approach isn't really suitable for these cases. </p> <p>&lt;insert image later&gt;</p>"},{"location":"random/image-auto-mesh/#back-to-square-one","title":"Back to Square One","text":"<p>The pizza method obviously doesn't work for all cases, but marching squares gives me too much high frequency details. How can we go about this? </p> <p>If the high frequency details is the only problem to have a clean contour, why not just filter them out? </p> <p>That's a really good observation. It's essentially downscaling the image and calculate the contour on a lower resolution image. It would be blocky for sure, but with proper interpolation, it can still be visually appealing. The bonus point is, using marching squares, we know that it won't produce self-intersecting contour and each edge length won't exceed the diagonal length of the grid. </p> <p>TODO</p>"},{"location":"random/image-auto-mesh/#meshing","title":"Meshing","text":"<p>Good contour, huh? But how to turn it into a triangle mesh? We couldn't just do a ear clipping triangulation, that's embarrassing. And bad triangulation leads to all kinds of texture stretching after deformation. </p>"},{"location":"random/image-auto-mesh/#poisson-disk-sampling","title":"Poisson Disk Sampling","text":""},{"location":"random/image-auto-mesh/#constrained-delaunay-triangulation","title":"Constrained Delaunay Triangulation","text":""},{"location":"random/ue5-landscape/","title":"Unreal Engine 5 Landscape","text":"<p>I've been wanted to try out the landscape tool in UE5. I am never a terrain guy, never got any sense and talent to make my own landscape like an artist does. But I am willing to try to do something new. The Unreal Engine provides a lot of different sophisticated tools with amazing graphics performance. This will be a great starting point for me.</p> <p>For this article, I will be mainly following this YouTube video that gives a solid foundation of how the landscape tool works in UE. Of course I won't blindly follow tutorials. Just for the sake of learning, I have used an AI tool to generate a landscape as my reference, and attempting to recreate this inside Unreal.</p> <p></p>"},{"location":"random/ue5-landscape/#landmass-tool","title":"Landmass Tool","text":"<p>This is an experimental tool in UE at the time of writing, so I have to enable it manually through the plugins. This provides the ability to model landscapes through blueprint, whatever that means, it's just a better tool than the traditional hand-painted terrain. </p> <p></p> <p></p> <p>I found out that it was also a non-destructive way of modeling landscape. The normal painting tools can be quite difficult to control. Since most of the time you won't know what will be the final look, moving landscape features and re-sculpting are very common while making maps. For instance, the following mountains are made with multiple blueprint brushes with additive blend mode. </p> <p></p>"},{"location":"random/ue5-landscape/#texturing","title":"Texturing","text":"<p>Of course gray mountains and basins are boring, time to add material to the landscape to preview how it looks. I am using the free plugin MW Landscape Auto Material available in the UE marketplace. As this is an automatic tool that splat textures onto the landscape based on the elevation in world space. Tuning the parameters is actually very easy and intuitive. I am able to quickly to set it up and did a screenshot of the progress I made so far.</p> <p> </p>"}]}