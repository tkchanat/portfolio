{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Tsz Kin Chan, Andy  Vancouver, BC</p> <p> tkchanat |  @tkchanat |  Andy Chan</p> <p>Rendering software engineer at Animal Logic Vancouver. I write any codes mostly related to both realtime and offline graphics. Fascinated in light transport and Monte Carlo path tracers. </p>"},{"location":"#notes","title":"Notes","text":""},{"location":"#monte-carlo-integration","title":"Monte Carlo Integration","text":""},{"location":"#projects","title":"Projects","text":"<p>Migrating from old webpage </p>"},{"location":"#experience","title":"Experience","text":"<p>Trademarks of the above are owned by their respective companies and publishers.</p>"},{"location":"#publication","title":"Publication","text":"<p>Can You See the Heat? A Null-scattering Approach for Refractive Volume Rendering - SIGGRAPH 2023</p> <p>Basile Fraboni, Tsz Kin Chan, Thibault Vergne, Jakub Jeziorski</p> <p>Presentation Session</p>"},{"location":"#education","title":"Education","text":"<p>Hong Kong University of Science and Technology, HK (2016 - 2020)</p> <p>Bachelor of Engineering, Computer Science</p> <ul> <li>Advanced Computer Graphics</li> <li>Applied Statistics and Linear Algebra</li> <li>Computer Organization and Operating System</li> <li>Data Structure and Algorithms</li> <li>Software Engineering</li> </ul> <p>KTH Royal Institue of Technology, Sweden (2019 Jan - June)</p> <p>Exchange Study, Master's Program</p> <ul> <li>Computer Graphics and Interaction</li> <li>Music Communication and Music Technology</li> <li>Machine Learning</li> </ul>"},{"location":"#skills","title":"Skills","text":""},{"location":"#tools-application","title":"Tools &amp; Application","text":"VSCode VisualStudio Git Unity3D UE4 Blender"},{"location":"#programming-languages","title":"Programming Languages","text":"C++ Rust Python C# Javascript"},{"location":"#graphics-api","title":"Graphics API","text":"Vulkan OpenGL DirectX 11 WebGL"},{"location":"notes/monte-carlo-integration/","title":"Monte Carlo Integration","text":""},{"location":"notes/monte-carlo-integration/#motivation","title":"Motivation","text":""},{"location":"notes/monte-carlo-integration/#quadrature-rules","title":"Quadrature rules","text":"<p>To integrate the function \\(I = \\int_{\\Omega}{f(x) dx}\\), where \\(\\Omega\\) is the domain of integration. If \\(f(x)\\) known to you and can be evaluated analytically, then you are good. But usually that's not the case, \\(f(x)\\) behaves like a blackbox (you give it an input and it spits out an output), it needs to be integrated numerically. </p> <p>Pretend you don't know the equation for the following function and you want to know its area bounded by it. In high school, we were taught that the integral can be approximated with infinitesimal rectangles, a.k.a. the rectangle rules. So let's put together a bunch of rectangles and crunch some numbers!</p> <p>As the number of rectangles grows, the closer it fills the target area. To which I can say with confidence the area is \\(\\pi/4\\) since it's obviously a quarter of a circle. </p> <p>There are more similar approaches to do integration and they are known as quatrature rules. The generalized form looks like this \\(\\hat{I} = \\sum_{i=1}^n w_i\\ f(x_i)\\). The weight \\(w_i\\) is defined differently in other rules, e.g. Midpoint rule, Trapezoid rule, Simpson's rule, etc. </p>"},{"location":"notes/monte-carlo-integration/#downside","title":"Downside","text":"<p>These kind of numerical approximations suffers from two major problems. </p>"},{"location":"notes/monte-carlo-integration/#high-frequency-signals","title":"High-frequency signals","text":"<p>Graphs like this can't be easily approximated with thick bars. To get closer to the real value, the bars must be really narrow which means more iterations to compute. </p>"},{"location":"notes/monte-carlo-integration/#high-dimensional-domains","title":"High-dimensional domains","text":"\\[ \\hat{I}=\\sum_{i_1=1}^n \\sum_{i_2=1}^n \\dots \\sum_{i_s=1}^n{w_{i_1} w_{i_2} \\dots w_{i_s} f(x_{i_1}, x_{i_2}, \\dots, x_{i_s})} \\] <p>where \\(s\\) is the dimension, \\(w_i\\) is the weights and \\(x_i\\) is the sample position in the domain. Which is always the case when solving light transport problems.</p>"},{"location":"notes/monte-carlo-integration/#definition","title":"Definition","text":"<p>As the name suggests, we can integrate functions with stochastic approaches through random sampling. With the Monte Carlo method, we can tap into the power of integrating arbitrary multi-dimensional functions!</p> \\[ F_N = \\frac{1}{N} \\sum_{i=1}^N{ \\frac{f(X_i)}{p(X_i)} } \\] <p>where \\(p(X_i)\\) is the probability density function (pdf). And we can verify that with sufficient samples, it will eventually converge to the expected value \\(I\\).</p> \\[ \\begin{align} E[F_N] &amp;= E[\\frac{1}{N}\\sum_{i=1}^N{\\frac{f(X_i)}{p(X_i)}}] \\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^N{\\int_{\\Omega}{\\frac{f(x)}{p(x)} p(x) d\\mu(x)}} \\\\ &amp;= \\int_{\\Omega}{f(x) d\\mu(x)} \\\\ &amp;= I \\end{align} \\] <p>It comes with the following benefits:</p> <ol> <li>It has a convergence rate of \\(O(\\frac{1}{\\sqrt{N}})\\) in any number of dimensions, which quadrature rule methods cannot achieve.</li> <li>Simplicity. All you need is two functions <code>sample()</code> and <code>eval()</code>, and occationally finding a suitable pdf.</li> </ol>"},{"location":"notes/monte-carlo-integration/#sampling-random-variables","title":"Sampling random variables","text":"<p>Generating uniform samples (e.g. \\(p(x)\\) is a constant) gurantees convergence but its rate is much slower. Ultimately, the samples should be drawn from a specific distribution such that most contribution to the integral is being extracted quickly from the domain. In other words, sampling carefully reduces the time to compute the ground-truth result.</p>"},{"location":"notes/monte-carlo-integration/#inverse-transform-sampling","title":"Inverse Transform Sampling","text":"<p>This is a method for generating random variables from a given probability distribution (pdf) by using its inverse cumulative distribution (cdf). Imagine the likelihood of picking a random variable \\(X\\) follows a normal distribution, and we want the samples to be drawn proportional to its likeliness. A cdf table is built by summing up the marginal distributions (It's totally fine that the pdf doesn't add up to 1, since the sample will be drawn from the range of cdf which can be normalized by its maximum value). Then uniform samples \\(U\\) are drawn in the inverse domain of cdf such that random variable \\(X\\) is picked with \\(X = cdf^{-1}(U)\\).</p> <p>Add 1 sample Add 10 samples Reset</p> <p>From the above example, we can see most samples are centered at the highest probability area while the tails on both sides will have lower sample count. Good thing with this method is that it can be easily extended to multi-dimensional cases, and stratifying samples in the domain helps improves exploring the entire domain. </p>"},{"location":"notes/monte-carlo-integration/#downside_1","title":"Downside","text":"<p>The pdf must be known, also building the cdf takes time and memory. And if the cdf cannot be inverted analytically, numerically computing the inverse mapping value (e.g. binary search) on every drawn samples is quite costly.</p> <p>Note: A more efficient sampling structure exists out there, and it's called the Alias Method, which samples can be drawn in constant \\(O(1)\\) time. // TODO: Make a page about this</p>"},{"location":"notes/monte-carlo-integration/#rejection-sampling","title":"Rejection Sampling","text":"<p>When the pdf is difficult to sample, we can instead sample from a simpler density \\(q\\) such that \\(p(x) \\le M q(x)\\), where \\(M\\) is a scaling constant. For instance, you want to integrate the area of an arbitrary shape but directly sampling the shape is hard. However, you know it's much easiler to draw samples from a box. So let's throw some dots onto our sample space!</p> <p>Start Reset</p> <p>The sampling space is defined as the tight bounding box of the shape, since drawing samples from a square is much simpler. We know the probability \\(p\\) of picking a sample point inside the shape is always less than the density \\(q\\) (it's completely enclosed within the bound), it's eligible to use this strategy to draw a sample. To draw one:</p> <ol> <li>Sample \\(X_i\\) according to \\(q\\) (draw a point inside the square)</li> <li>Sample \\(U_i\\) uniformly on \\([0, 1]\\)</li> <li>If \\(U_i \\le p(X_i) / (Mq(X_i))\\), return the sample \\(X_i\\)</li> <li>Else, repeat 1</li> </ol> <p>In the above case, \\(p(X_i)\\) is \\(\\frac{1}{area}\\) when the point is drawn inside the shape else zero, and \\(q(X_i)\\) is always \\(\\frac{1}{64}\\) because we are uniformly sampling a 8x8 square. Given that \\(U_i\\) has a trivial probability of being 0, we can safely assume that all valid samples \\(X_i\\) are located inside the shape. Thus we know they are good samples.</p>"},{"location":"notes/monte-carlo-integration/#importance-sampling","title":"Importance Sampling","text":"<p>\\(N=\\) \\(\\int{f(x)}dx =\\) Approx \\(=\\) Start Reset</p> <ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/sampling-techniques/","title":"Sampling Techniques","text":"<p>Puseudo-random noise usually don't produce good uniform samples. It often produces clusters and void within the sample space. That means the sample space is not well explored, either wasting samples on similar areas or even complete ignoring some subregions. And this scales to any dimensions.</p>"},{"location":"notes/sampling-techniques/#stratified-sampling","title":"Stratified Sampling","text":"<p>Instead of recklessly scatter points around, how about subdiving the domain \\(\\Omega\\) into non-overlapping regions \\(\\Omega_1, \\dots, \\Omega_n\\). Noted that the union of them must cover the whole domain.</p> \\[ \\bigcup_{i=1}^n{\\ \\Omega_i} = \\Omega \\] <p>This works pretty well in low-dimensional integration problems. However unfortunately, this suffers the same problem as the quadrature rules where it won't perform well in high frequency signals.</p>"},{"location":"notes/sampling-techniques/#low-discrepancy-sequences","title":"Low-Discrepancy Sequences","text":"<p>If randomness are difficult to control, how about herding the points to position in a deterministic pattern such that they are almost equal distance to each other? In other words, how do we get an equidistribution of samples?</p>"},{"location":"notes/sampling-techniques/#defining-discrepancy","title":"Defining Discrepancy","text":"<p>Imagine a \\(s\\)-dimentional unit cube \\(\\mathbb{I}^s = [0, 1)^s\\), with a point set \\(P = {x_1, x_2, \\dots, x_N} \\in \\mathbb{I}^s\\). We define the point set discrepancy \\(\\mathcal{D}(J, P)\\) as follows:</p> \\[ \\mathcal{D}(J, P) = \\left|\\frac{A(J)}{N} - V(J)\\right| \\] <p>You can think of \\(\\mathcal{D}(J, P)\\) as the proportion of points inside a sub-interval \\(J\\), where \\(A(J)\\) is the number of points \\(x_i \\in J\\) and \\(V(J)\\) is the volume of \\(J\\). </p> <p>The worst-case discrepancy is called the star-discrepancy and is defined as:</p> \\[ \\mathcal{D^*}(N)=\\sup_{J\\in\\mathbb{I^s}}{|\\mathcal{D}(J;P)|} \\] <p>A good sequence should minimize such star-discrepancy \\(\\mathcal{D^*}(N)\\) to be qualified as low-discrepancy. Perhaps it's easier to visualize the terms in a diagram.</p> <p>\\(N=\\) \\(\\mathcal{D}(A, P) =|\\)\\(\\frac{A(A)}{N}-V(A)\\)\\(|=\\) \\(\\mathcal{D}(B, P) =|\\)\\(\\frac{A(B)}{N}-V(B)\\)\\(|=\\) \\(\\mathcal{D}(C, P) =|\\)\\(\\frac{A(C)}{N}-V(C)\\)\\(|=\\) \\(\\mathcal{D^*}(P) =\\)</p> <p>Generate Point Set Reset</p>"},{"location":"notes/sampling-techniques/#halton-sequence","title":"Halton Sequence","text":"<p>One of the well-known low discrepancy sequences is generated using the radical inverse of numbers. They are called radical inverse sequence, and is defined as:</p> \\[ \\phi_b(i) = \\sum_{k\\ge 0}{d_{i,k}\\ b^{-1-k}} \\] <p>where \\(b\\) is the base and \\(d_k\\) is the \\(k\\)-th digit in the \\(b\\)-ary expansion of \\(n\\). To generate a sequence for \\(b=2\\), first represent the natural numbers in binary, then revert the digits and take its inverse. i.e.</p> \\[ \\begin{align} &amp;\\quad \\phi_2(1),\\phi_2(2),\\phi_2(3),\\phi_2(4),\\phi_2(5),\\phi_2(6),\\dots \\\\ &amp;=0.1_2, 0.01_2, 0.11_2, 0.001_2, 0.101_2, 0.011_2,\\dots \\\\ &amp;=\\frac12,\\frac14,\\frac34,\\frac18,\\frac58,\\frac38,\\dots \\\\ \\end{align} \\] <p>This is also known as the van der Corput sequence, a specialized one-dimensional radical inverse sequence. The generalized form is called the Halton sequence, that is scalable to higher dimensions. To generate points in \\(N\\)-dimension, simply pick a different base from the prim number series \\({2, 3, 5, 7, 11, \\dots}\\). Scratchapixel has a more in-depth explanation, feel free to give it a read! </p> <p>Start Reset</p>"},{"location":"notes/sampling-techniques/#sobol-sequence","title":"Sobol Sequence","text":""},{"location":"notes/sampling-techniques/#progressive-multi-jittered-sample-sequence","title":"Progressive Multi-Jittered Sample Sequence","text":"<ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> <li> <p>Dalal, I., Stefan, D., &amp; Harwayne-Gidansky, J. (2008). Low discrepancy sequences for Monte Carlo simulations on reconfigurable platforms. In 2008 International Conference on Application-Specific Systems, Architectures and Processors (pp. 108\u2013113).\u00a0\u21a9</p> </li> <li> <p>van der Corput, J.G. (1935), \"Verteilungsfunktionen (Erste Mitteilung)\" (PDF), Proceedings of the Koninklijke Akademie van Wetenschappen te Amsterdam (in German), 38: 813\u2013821, Zbl 0012.34705\u00a0\u21a9</p> </li> <li> <p>Christensen, P., Kensler, A., &amp; Kilpatrick, C. (2018). Progressive Multi-Jittered Sample Sequences. Computer Graphics Forum.\u00a0\u21a9</p> </li> </ol>"}]}