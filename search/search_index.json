{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Andy (Tsz Kin) Chan <p> tkchanat |  @tkchanat |  Andy Chan |  Vancouver, BC</p> <p>Rendering software engineer at Animal Logic Vancouver. I write codes in both realtime and offline graphics. Fascinated by light transport and Monte Carlo path tracers. </p>"},{"location":"#projects","title":"Projects","text":"<p>Migrating from old webpage </p>"},{"location":"#experience","title":"Experience","text":"<p>* Trademarks of the above are owned by their respective companies and publishers.</p>"},{"location":"#publication","title":"Publication","text":"<p>Can You See the Heat? A Null-scattering Approach for Refractive Volume Rendering - SIGGRAPH 2023</p> <p>Basile Fraboni, Tsz Kin Chan, Thibault Vergne, Jakub Jeziorski</p> <p>[Project Page] | [Paper]</p>"},{"location":"#education","title":"Education","text":"<p>Hong Kong University of Science and Technology, HK (2016 - 2020)</p> <p>Bachelor of Engineering, Computer Science</p> <ul> <li>Advanced Computer Graphics</li> <li>Applied Statistics and Linear Algebra</li> <li>Computer Organization and Operating System</li> <li>Data Structure and Algorithms</li> <li>Software Engineering</li> </ul> <p>KTH Royal Institue of Technology, Sweden (2019 Jan - June)</p> <p>Exchange Study, Master's Program</p> <ul> <li>Computer Graphics and Interaction</li> <li>Music Communication and Music Technology</li> <li>Machine Learning</li> </ul>"},{"location":"#skills","title":"Skills","text":"Tools &amp; Application VSCode VisualStudio Git USD Unity3D UE4 Blender Programming Languages C++ Rust Python C# Javascript Graphics API Vulkan OpenGL DirectX 11 WebGL"},{"location":"notes/microfacet/","title":"Microfacet Theory","text":"<p>There is a well-written paper by Eric Heitz1 explaining every details about the microfacet theory. I highly recommend everyone to read it to get a thorough understanding from beginning to end. The stuff I write is just an abstract plus my own understanding (prone to misinformation). I am happy to correct this page anytime. </p>"},{"location":"notes/microfacet/#radiance-on-surface","title":"Radiance on Surface","text":"<p>Here is the definition of radiance from wikipedia.</p> <p>In radiometry, radiance is the radiant flux emitted, reflected, transmitted or received by a given surface, per unit solid angle per unit projected area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>It matches the one defined in Veach's thesis, which is written as:</p> \\[ \\begin{align*} L(\\omega, \\mathbf{x}) &amp;= \\frac{d^2\\Phi(\\omega)}{dA^\\bot_{\\omega}(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\ d\\sigma(\\omega)}\\\\ &amp;= \\frac{d^2\\Phi(\\omega)}{dA(\\mathbf{x})\\ d\\sigma^\\bot_{\\omega}(\\omega)} \\end{align*} \\] <p>The \\(A^\\bot_{\\omega}\\) here means the area projected from the hypothetical surface onto the observed direction \\(\\omega\\). For a real surface, the projected area measure \\(dA^\\bot_{\\omega}\\) in a real surface can be written as \\(\\left|\\omega\\cdot \\mathbf{n}\\right|dA(\\mathbf{x})\\). Even better, the dot product \\(\\left|\\omega\\cdot \\mathbf{n}\\right|\\) can be transfered into the solid angle measure \\((\\sigma_\\omega \\rightarrow \\sigma^\\bot_{\\omega})\\), which can be beneficial sometimes if we want to leverage that property during integration, e.g. Stratified sampling of projected spherical caps, EGSR 2018.</p> <p>Extending this to a microfacet model, where every micronormals \\(\\omega_m\\) has its own direction, we need to integrate that over a small patch of surface \\(\\mathcal{M}\\). </p> \\[ L(\\omega_o, \\mathcal{M}) = \\frac{\\int_\\mathcal{M}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{M})} \\] <p>For every infinitesimal points \\(\\mathbf{x}\\in\\mathcal{M}\\), its local radiance towards the observing direction is \\(A^\\bot_{\\omega_o}L(\\omega_o,\\mathbf{x})\\). So the integrated result will be the total amount of watts per steradian \\([W\\cdot sr^{-1}]\\) that is radiating off to the observing direction \\(\\omega_o\\). The purpose of the denominator \\(A^\\bot_{\\omega_o}(\\mathcal{M})\\) which represents the total projected area of \\(\\mathcal{M},\\) is here merely to normalize the entire thing back to per unit area \\([W\\cdot sr^{-1}\\cdot m^{-2}]\\).</p> <p>Fun little fact, for marcosurface \\(\\mathcal{G}\\) which its normal doesn't vary within the domain, it's radiance is no difference than the original definition of radiance.</p> \\[ \\begin{align*} L(\\omega_o, \\mathcal{G}) &amp;= \\frac{\\int_\\mathcal{G}{A^\\bot_{\\omega_o}(\\mathbf{x})\\ L(\\omega_o, \\mathbf{x})\\ d\\mathbf{x}}}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= \\frac{A^\\bot_{\\omega_o}(\\mathcal{G})\\ L(\\omega_o, \\mathbf{x})}{A^\\bot_{\\omega_o}(\\mathcal{G})}\\\\ &amp;= L(\\omega_o, \\mathbf{x}) \\end{align*} \\]"},{"location":"notes/microfacet/#normal-distribution","title":"Normal Distribution","text":"<p>Spatial integral isn't ideal to work with because the whole microfacet theory is based on a statistical model, it's not necessarily tied to an actual spatial representation. That being said, we need a way to convert that integral into the solid angle domain. Luckily, the distribution of normals is able to provide just that.</p> \\[ D(\\omega) = \\int_\\mathcal{M}{\\delta_\\omega(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x}} \\] <p>It is expressed in square meters per steradian \\([m^2\\cdot sr^{-1}]\\), representing the density of normals that aligns a given direction \\(\\omega\\) over the surface \\(\\mathcal{M}\\), hence the dirac delta function \\(\\delta_\\omega\\). To better understand this magical function, imagine counting the number of micronormals in the patch that are pointing to the direction \\(\\omega\\) within the domain space \\(\\mathcal{M}\\). Its sum will be the area covering the selected population of normals, and this is where the area term \\(m^2\\) comes from. This adds on top of the fact that dirac delta always has the inverse dimension of its argument, which is in \\(sr\\) here, so its unit is per steradian \\(sr^{-1}\\). </p> <p>Let's talk about an interesting property of the integral of normal distribution. Over any arbitrary solid angle region \\(\\Omega'\\subset\\Omega\\) on the unit sphere, it always gives the total covered area whose normals lie in \\(\\Omega'\\).</p> \\[ \\int_\\mathcal{M'}1\\ d\\mathbf{x} = \\int_{\\Omega'} D(\\omega_m)\\ d\\omega_m \\] <p>This makes \\(\\int_\\Omega D(\\omega_m)\\ d\\omega_m\\) the total area of the microsurface \\(\\mathcal{M}\\). Magical, right? Now any attempts of integrating a function of the microsurface normal \\(\\omega_m\\) spatially over \\(\\mathcal{M}\\) can be converted into a statistial integral:</p> \\[ \\int_\\mathcal{M}f(\\omega_m(\\mathbf{x}))\\ d\\mathbf{x} = \\int_\\Omega f(\\omega_m)\\ D(\\omega_m)\\ d\\omega_m \\]"},{"location":"notes/microfacet/#geometric-attenuation","title":"Geometric Attenuation","text":"<p>\\(G_1(\\omega_o, \\omega_m)\\) is the statistical masking function of micronormal \\(\\omega_m\\). </p> <p>\\(G_1(\\omega_i, \\omega_m)\\) is the statistical shadowing function of micronormal \\(\\omega_m\\)</p> <p>\\(G(\\omega_i, \\omega_o)\\) is the shadowing-masking term</p>"},{"location":"notes/microfacet/#various-models","title":"Various Models","text":""},{"location":"notes/microfacet/#ggx","title":"GGX","text":"<p>GGX2 is a well-known geometry-based microfacet BSDF model for rough surfaces. </p> \\[D(\\mathbf{h})=\\frac{\\alpha^2\\chi^+(\\left&lt;\\mathbf{h}\\cdot\\mathbf{n}\\right&gt;)}{\\pi cos^4\\theta_\\mathbf{h}(\\alpha^2+tan^2\\theta_\\mathbf{h})^2}\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=\\chi^+\\left(\\frac{\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{v}\\cdot\\mathbf{n}\\right&gt;}\\right)\\quad\\frac{2}{1+\\sqrt{1+\\alpha^2tan^2\\theta_\\mathbf{v}}}\\]"},{"location":"notes/microfacet/#schlicks-approximation","title":"Schlick's Approximation","text":"<p>Christophe Schlick proposed an inexpensive and efficient model3 in attempt to capture the microfacet properties. This is probably the most popular go-to model aside from the Cook-Torrance model, usually used in real-time graphics since it has less mathematical operations. Both \\(F\\) and \\(G\\) terms are inspired from the Cook-Torrance model, while the \\(D\\) term is derived from the Beckmann distribution.</p> \\[F(\\mathbf{v},\\mathbf{h})=f_0+(1-f_0)(1-\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;)^5\\] \\[D(\\mathbf{h})=\\frac{\\alpha^3x}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(\\alpha x^2-x^2+\\alpha^2)^2},\\quad x=\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;+\\alpha-1\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=G_1(\\mathbf{l})\\ G_1(\\mathbf{v}),\\quad G_1(\\mathbf{v})=\\frac{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(1-k)+k},\\quad k=\\sqrt{\\frac{\\pi}{2\\alpha^2}}\\]"},{"location":"notes/microfacet/#unreal-4-approximation","title":"Unreal 4 Approximation","text":"<p>Epic Games4 mostly adopts the formulations of \\(F\\) and \\(G\\) from the Schlick's model with slight modifications, but picked the \\(D\\) in the Disney's GGX model. These are obviously an approximation on top of an approximation. Every decisions made here are sacrificing minor visual error for faster computations.</p> \\[F(\\mathbf{v},\\mathbf{h})=f_0+(1-f_0)\\cdot2^{(-5.55473\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;-6.98316)\\left&lt;\\mathbf{v}\\cdot\\mathbf{h}\\right&gt;}\\] \\[D(\\mathbf{h})=\\frac{\\alpha^2}{\\pi(\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;^2(\\alpha^2-1)+1)^2}\\] \\[G(\\mathbf{l},\\mathbf{v},\\mathbf{h})=G_1(\\mathbf{l})\\ G_1(\\mathbf{v}),\\quad G_1(\\mathbf{v})=\\frac{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;}{\\left&lt;\\mathbf{n}\\cdot\\mathbf{h}\\right&gt;(1-k)+k},\\quad k=\\frac{(\\alpha+1)^2}{8}\\] <ol> <li> <p>Heitz E. (2014). https://jcgt.org/published/0003/02/03/paper.pdf. Journal of Computer Graphics Techniques, 3(2), 32-91. \u21a9</p> </li> <li> <p>Walter B., Marschner S. R., Li H., &amp; Torrance K. E. (2007, June). Microfacet models for refraction through rough surfaces. In Proceedings of the 18th Eurographics conference on Rendering Techniques (pp. 195-206). \u21a9</p> </li> <li> <p>Schlick C. (1994, August). An inexpensive BRDF model for physically\u2010based rendering. In Computer graphics forum (Vol. 13, No. 3, pp. 233-246). Edinburgh, UK: Blackwell Science Ltd. \u21a9</p> </li> <li> <p>Karis B, Epic Games. (2013). Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 4(3), 1. \u21a9</p> </li> </ol>"},{"location":"notes/monte-carlo-integration/","title":"Monte Carlo Integration","text":""},{"location":"notes/monte-carlo-integration/#motivation","title":"Motivation","text":""},{"location":"notes/monte-carlo-integration/#quadrature-rules","title":"Quadrature rules","text":"<p>To integrate the function \\(I = \\int_{\\Omega}{f(x) dx}\\), where \\(\\Omega\\) is the domain of integration. If \\(f(x)\\) known to you and can be evaluated analytically, then you are good. But usually that's not the case, \\(f(x)\\) behaves like a blackbox (you give it an input and it spits out an output), it needs to be integrated numerically. </p> <p>Pretend you don't know the equation for the following function and you want to know its area bounded by it. In high school, we were taught that the integral can be approximated with infinitesimal rectangles, a.k.a. the rectangle rules. So let's put together a bunch of rectangles and crunch some numbers!</p> <p>As the number of rectangles grows, the closer it fills the target area. To which I can say with confidence the area is \\(\\pi/4\\) since it's obviously a quarter of a circle. </p> <p>There are more similar approaches to do integration and they are known as quatrature rules. The generalized form looks like this \\(\\hat{I} = \\sum_{i=1}^n w_i\\ f(x_i)\\). The weight \\(w_i\\) is defined differently in other rules, e.g. Midpoint rule, Trapezoid rule, Simpson's rule, etc. </p>"},{"location":"notes/monte-carlo-integration/#downside","title":"Downside","text":"<p>These kind of numerical approximations suffers from two major problems. </p>"},{"location":"notes/monte-carlo-integration/#high-frequency-signals","title":"High-frequency signals","text":"<p>Graphs like this can't be easily approximated with thick bars. To get closer to the real value, the bars must be really narrow which means more iterations to compute. </p>"},{"location":"notes/monte-carlo-integration/#high-dimensional-domains","title":"High-dimensional domains","text":"\\[ \\hat{I}=\\sum_{i_1=1}^n \\sum_{i_2=1}^n \\dots \\sum_{i_s=1}^n{w_{i_1} w_{i_2} \\dots w_{i_s} f(x_{i_1}, x_{i_2}, \\dots, x_{i_s})} \\] <p>where \\(s\\) is the dimension, \\(w_i\\) is the weights and \\(x_i\\) is the sample position in the domain. Which is always the case when solving light transport problems.</p>"},{"location":"notes/monte-carlo-integration/#definition","title":"Definition","text":"<p>As the name suggests, we can integrate functions with stochastic approaches through random sampling. With the Monte Carlo method, we can tap into the power of integrating arbitrary multi-dimensional functions!</p> \\[ F_N = \\frac{1}{N} \\sum_{i=1}^N{ \\frac{f(X_i)}{p(X_i)} } \\] <p>where \\(p(X_i)\\) is the probability density function (pdf). And we can verify that with sufficient samples, it will eventually converge to the expected value \\(I\\).</p> \\[ \\begin{align} E[F_N] &amp;= E[\\frac{1}{N}\\sum_{i=1}^N{\\frac{f(X_i)}{p(X_i)}}] \\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^N{\\int_{\\Omega}{\\frac{f(x)}{p(x)} p(x) d\\mu(x)}} \\\\ &amp;= \\int_{\\Omega}{f(x) d\\mu(x)} \\\\ &amp;= I \\end{align} \\] <p>It comes with the following benefits:</p> <ol> <li>It has a convergence rate of \\(O(\\frac{1}{\\sqrt{N}})\\) in any number of dimensions, which quadrature rule methods cannot achieve.</li> <li>Simplicity. All you need is two functions <code>sample()</code> and <code>eval()</code>, and occationally finding a suitable pdf.</li> </ol>"},{"location":"notes/monte-carlo-integration/#sampling-random-variables","title":"Sampling random variables","text":"<p>Generating uniform samples (e.g. \\(p(x)\\) is a constant) gurantees convergence but its rate is much slower. Ultimately, the samples should be drawn from a specific distribution such that most contribution to the integral is being extracted quickly from the domain. In other words, sampling carefully reduces the time to compute the ground-truth result.</p>"},{"location":"notes/monte-carlo-integration/#inverse-transform-sampling","title":"Inverse Transform Sampling","text":"<p>This is a method for generating random variables from a given probability distribution (pdf) by using its inverse cumulative distribution (cdf). Imagine the likelihood of picking a random variable \\(X\\) follows a normal distribution, and we want the samples to be drawn proportional to its likeliness. A cdf table is built by summing up the marginal distributions (It's totally fine that the pdf doesn't add up to 1, since the sample will be drawn from the range of cdf which can be normalized by its maximum value). Then uniform samples \\(U\\) are drawn in the inverse domain of cdf such that random variable \\(X\\) is picked with \\(X = cdf^{-1}(U)\\).</p> <p>Add 1 sample Add 10 samples Reset</p> <p>From the above example, we can see most samples are centered at the highest probability area while the tails on both sides will have lower sample count. Good thing with this method is that it can be easily extended to multi-dimensional cases, and stratifying samples in the domain helps improves exploring the entire domain. </p>"},{"location":"notes/monte-carlo-integration/#downside_1","title":"Downside","text":"<p>The pdf must be known, also building the cdf takes time and memory. And if the cdf cannot be inverted analytically, numerically computing the inverse mapping value (e.g. binary search) on every drawn samples is quite costly.</p> <p>Note: A more efficient sampling structure exists out there, and it's called the Alias Method, which samples can be drawn in constant \\(O(1)\\) time. // TODO: Make a page about this</p>"},{"location":"notes/monte-carlo-integration/#rejection-sampling","title":"Rejection Sampling","text":"<p>When the pdf is difficult to sample, we can instead sample from a simpler density \\(q\\) such that \\(p(x) \\le M q(x)\\), where \\(M\\) is a scaling constant. For instance, you want to integrate the area of an arbitrary shape but directly sampling the shape is hard. However, you know it's much easiler to draw samples from a box. So let's throw some dots onto our sample space!</p> <p>Start Reset</p> <p>The sampling space is defined as the tight bounding box of the shape, since drawing samples from a square is much simpler. We know the probability \\(p\\) of picking a sample point inside the shape is always less than the density \\(q\\) (it's completely enclosed within the bound), it's eligible to use this strategy to draw a sample. To draw one:</p> <ol> <li>Sample \\(X_i\\) according to \\(q\\) (draw a point inside the square)</li> <li>Sample \\(U_i\\) uniformly on \\([0, 1]\\)</li> <li>If \\(U_i \\le p(X_i) / (Mq(X_i))\\), return the sample \\(X_i\\)</li> <li>Else, repeat 1</li> </ol> <p>In the above case, \\(p(X_i)\\) is \\(\\frac{1}{area}\\) when the point is drawn inside the shape else zero, and \\(q(X_i)\\) is always \\(\\frac{1}{64}\\) because we are uniformly sampling a 8x8 square. Given that \\(U_i\\) has a trivial probability of being 0, we can safely assume that all valid samples \\(X_i\\) are located inside the shape. Thus we know they are good samples.</p>"},{"location":"notes/monte-carlo-integration/#importance-sampling","title":"Importance Sampling","text":"<p>\\(N=\\) \\(\\int{f(x)}dx =\\) Approx \\(=\\) Start Reset</p>"},{"location":"notes/monte-carlo-integration/#multiple-importance-sampling","title":"Multiple Importance Sampling","text":"<p>Next event estimation can be seen as a multiple importance sampling approach of integrating the radiance since it combines two sampling strategies, BSDF sampling and light sampling, often called as direct and indirect lighting. </p>"},{"location":"notes/monte-carlo-integration/#bsdf-sampling","title":"BSDF Sampling","text":"\\[ p(w_i') \\propto f_s(\\mathbf{x}', w_i' \\rightarrow w_o') \\] <p>Depending on the surface properties, there are certain directions (\\(w_i'\\)) the BSDF favors after an interaction. To get the more contribution from the function, samples have to be drawn proportional to the \\(f_s\\)'s shape. Because BSDF samples are drawn inside the solid angle domain, the probability \\(p(w_i')\\) is also measured in solid angle.</p>"},{"location":"notes/monte-carlo-integration/#light-sampling","title":"Light Sampling","text":"<p>In case when BSDF failed to find a significant contribution, other words the outgoing ray direction missed the light source, light sampling then comes into play to provide as a backup strategy.</p> \\[ L_o(\\mathbf{x}'\\rightarrow\\mathbf{x}'') = \\int_{\\mathcal{M}}{f_s(\\mathbf{x}\\rightarrow\\mathbf{x}'\\rightarrow\\mathbf{x}'')L_e(\\mathbf{x}\\rightarrow\\mathbf{x}')G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')dA(\\mathbf{x})} \\] <p>\\(G(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\) often refers as the geometric term, which was introduced because of change of variables. When changing from projected solid angle \\(d\\sigma^\\bot(w_i')\\) to area measure \\(dA(\\mathbf{x})\\), it is required to have this term to normalize the integral:</p> \\[ G(\\mathbf{x}\\leftrightarrow\\mathbf{x}') = V(\\mathbf{x}\\leftrightarrow\\mathbf{x}')\\frac{cos(\\theta_o)cos(\\theta_i')}{||\\mathbf{x}-\\mathbf{x}'||^2} \\] <p>To importance sample the light instead of solid angle, the density \\(p(\\mathbf{x})\\) is predetermined since it is just the probability of picking a point on the manifold surface, i.e. \\(p(\\mathbf{x})=\\frac{1}{Area}\\). </p>"},{"location":"notes/monte-carlo-integration/#transformation-of-space-conversion-of-domain","title":"Transformation of Space / Conversion of Domain","text":"\\[ p(\\mathbf{x})=p(w_i')\\frac{d\\sigma^\\bot(w_i')}{dA(\\mathbf{x})}=p(w_i')\\frac{|cos(\\theta_o)cos(\\theta_i')|}{||\\mathbf{x}-\\mathbf{x}'||^2} \\] <ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/sampling-techniques/","title":"Sampling Techniques","text":"<p>Puseudo-random noise usually don't produce good uniform samples. It often produces clusters and void within the sample space. That means the sample space is not well explored, either wasting samples on similar areas or even complete ignoring some subregions. And this scales to any dimensions.</p>"},{"location":"notes/sampling-techniques/#stratified-sampling","title":"Stratified Sampling","text":"<p>Instead of recklessly scatter points around, how about subdiving the domain \\(\\Omega\\) into non-overlapping regions \\(\\Omega_1, \\dots, \\Omega_n\\). Noted that the union of them must cover the whole domain.</p> \\[ \\bigcup_{i=1}^n{\\ \\Omega_i} = \\Omega \\] <p>This works pretty well in low-dimensional integration problems. However unfortunately, this suffers the same problem as the quadrature rules where it won't perform well in high frequency signals.</p>"},{"location":"notes/sampling-techniques/#low-discrepancy-sequences","title":"Low-Discrepancy Sequences","text":"<p>If randomness are difficult to control, how about herding the points to position in a deterministic pattern such that they are almost equal distance to each other? In other words, how do we get an equidistribution of samples?</p>"},{"location":"notes/sampling-techniques/#defining-discrepancy","title":"Defining Discrepancy","text":"<p>Imagine a \\(s\\)-dimentional unit cube \\(\\mathbb{I}^s = [0, 1)^s\\), with a point set \\(P = {x_1, x_2, \\dots, x_N} \\in \\mathbb{I}^s\\). We define the point set discrepancy \\(\\mathcal{D}(J, P)\\) as follows:</p> \\[ \\mathcal{D}(J, P) = \\left|\\frac{A(J)}{N} - V(J)\\right| \\] <p>You can think of \\(\\mathcal{D}(J, P)\\) as the proportion of points inside a sub-interval \\(J\\), where \\(A(J)\\) is the number of points \\(x_i \\in J\\) and \\(V(J)\\) is the volume of \\(J\\). </p> <p>The worst-case discrepancy is called the star-discrepancy and is defined as:</p> \\[ \\mathcal{D^*}(N)=\\sup_{J\\in\\mathbb{I^s}}{|\\mathcal{D}(J;P)|} \\] <p>A good sequence should minimize such star-discrepancy \\(\\mathcal{D^*}(N)\\) to be qualified as low-discrepancy. Perhaps it's easier to visualize the terms in a diagram.</p> <p>\\(N=\\) \\(\\mathcal{D}(A, P) =|\\)\\(\\frac{A(A)}{N}-V(A)\\)\\(|=\\) \\(\\mathcal{D}(B, P) =|\\)\\(\\frac{A(B)}{N}-V(B)\\)\\(|=\\) \\(\\mathcal{D}(C, P) =|\\)\\(\\frac{A(C)}{N}-V(C)\\)\\(|=\\) \\(\\mathcal{D^*}(P) =\\)</p> <p>Generate Point Set Reset</p>"},{"location":"notes/sampling-techniques/#halton-sequence","title":"Halton Sequence","text":"<p>One of the well-known low discrepancy sequences is generated using the radical inverse of numbers. They are called radical inverse sequence, and is defined as:</p> \\[ \\phi_b(i) = \\sum_{k\\ge 0}{d_{i,k}\\ b^{-1-k}} \\] <p>where \\(b\\) is the base and \\(d_k\\) is the \\(k\\)-th digit in the \\(b\\)-ary expansion of \\(n\\). To generate a sequence for \\(b=2\\), first represent the natural numbers in binary, then revert the digits and take its inverse. i.e.</p> \\[ \\begin{align} &amp;\\quad \\phi_2(1),\\phi_2(2),\\phi_2(3),\\phi_2(4),\\phi_2(5),\\phi_2(6),\\dots \\\\ &amp;=0.1_2, 0.01_2, 0.11_2, 0.001_2, 0.101_2, 0.011_2,\\dots \\\\ &amp;=\\frac12,\\frac14,\\frac34,\\frac18,\\frac58,\\frac38,\\dots \\\\ \\end{align} \\] <p>This is also known as the van der Corput sequence, a specialized one-dimensional radical inverse sequence. The generalized form is called the Halton sequence, that is scalable to higher dimensions. To generate points in \\(N\\)-dimension, simply pick a different base from the prim number series \\({2, 3, 5, 7, 11, \\dots}\\). Scratchapixel has a more in-depth explanation, feel free to give it a read! </p> <p>Start Reset</p>"},{"location":"notes/sampling-techniques/#sobol-sequence","title":"Sobol Sequence","text":""},{"location":"notes/sampling-techniques/#progressive-multi-jittered-sample-sequence","title":"Progressive Multi-Jittered Sample Sequence","text":"<ol> <li> <p>Veach, E. (1997). Robust Monte Carlo Methods for Light Transport Simulation. (Doctoral dissertation, Stanford University).\u00a0\u21a9</p> </li> <li> <p>Dalal, I., Stefan, D., &amp; Harwayne-Gidansky, J. (2008). Low discrepancy sequences for Monte Carlo simulations on reconfigurable platforms. In 2008 International Conference on Application-Specific Systems, Architectures and Processors (pp. 108\u2013113).\u00a0\u21a9</p> </li> <li> <p>van der Corput, J.G. (1935), \"Verteilungsfunktionen (Erste Mitteilung)\" (PDF), Proceedings of the Koninklijke Akademie van Wetenschappen te Amsterdam (in German), 38: 813\u2013821, Zbl 0012.34705\u00a0\u21a9</p> </li> <li> <p>Christensen, P., Kensler, A., &amp; Kilpatrick, C. (2018). Progressive Multi-Jittered Sample Sequences. Computer Graphics Forum.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/simulation/force-based/","title":"Force Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/impulse-based/","title":"Impulse Based Simulation","text":"<p>TODO</p>"},{"location":"notes/simulation/position-based/","title":"Position Based Simulation","text":"<p>As oppose to any other rigidbody simulations, force-based and impulse-based that deals with velocity and acceleration calculations, position-based simulation attempts to minimize the constrains on the position domain down to particle level. The main benefits of PBD is that it's not suppose to suffer from overshooting problem. http://mmacklin.com/EG2015PBD.pdf</p>"},{"location":"notes/simulation/position-based/#problem","title":"Problem","text":"<p>Given a set of \\(M\\) constrains (basically means there are \\(M\\) equality or inequality equations to satisfy), we need to solve for \\(N\\times\\mathbb{R}^3\\) unknowns to resolve our final positions. Most of the time, the number of constraints won't match the number of unknowns we are solving (i.e. \\(M \\neq N\\)). This means if the problem was a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), the solution can't be obtained easily by inverting the matrix and solve for \\(\\mathbf{x}\\). Not to mention the system we are solving won't necessarily be linear, for example a simple distance constraint \\(c(\\mathbf{x}_1, \\mathbf{x}_2)=\\left|\\mathbf{x}_1-\\mathbf{x}_2\\right|^2-d^2\\) alone is a non-linear equation.</p> <p>Thus, it all boils down to a complex problem of finding a set of positions \\(\\mathbf{x}\\) that minimize if not solved the constrained system:</p> \\[ C(\\mathbf{x}) = \\left\\{\\\\ \\begin{array}\\\\ c_1(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_N) \\succ 0\\\\ \\cdots\\\\ c_M(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_N) \\succ 0\\\\ \\end{array}\\\\ \\right. \\] <p>Where \\(\\mathbf{x}\\) is the concatenation of \\(N\\times\\mathbb{R}^3\\) position unknowns we are trying to solve, and the symbol \\(\\succ\\) denotes either \\(=\\) or \\(\\geq\\). As said, there won't be closed form solution because the system is neither symmetric nor linear. Our best bet is to apply an iterative solver to minimize the system after a fixed amount of iterations and hope for the best that the approximated result will satisfy all our constraints. </p>"},{"location":"notes/simulation/position-based/#algorithm","title":"Algorithm","text":"<p>The main position based dynamics (PBD) algorithm can be split into three different stages: prediction, constraint projection, and post-solve updates.</p> \\[ \\begin{align} &amp;\\mathbf{while}\\ \\text{simulating}\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow \\mathbf{v}_i + \\mathbf{f}_{ext}\\Delta t\\\\ &amp;\\quad\\quad \\mathbf{p}_i \\leftarrow \\mathbf{x}_i + \\mathbf{v}_i\\Delta t\\\\ &amp;\\quad \\mathbf{for} \\text{ all constraints } C\\\\ &amp;\\quad\\quad solve(C, \\Delta t)\\\\ &amp;\\quad \\mathbf{for} \\text{ all particles } i\\\\ &amp;\\quad\\quad \\mathbf{v}_i \\leftarrow (\\mathbf{x}_i - \\mathbf{p}_i) \\frac{1}{\\Delta t}\\\\ &amp;\\quad\\quad \\mathbf{x}_i \\leftarrow \\mathbf{p}_i \\end{align} \\]"},{"location":"notes/simulation/position-based/#result","title":"Result","text":""}]}